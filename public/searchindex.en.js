var relearn_searchindex = [
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "OpenShift Prerequisites Before installing OpenShift, ensure the following prerequisites are met for your environment.\n1. Red Hat Account A Red Hat account is required to download the installer, access the pull secret, and manage subscriptions.\n👉 Create a Red Hat Account\n2. OpenShift Installer Download the OpenShift installer appropriate for your target environment (bare metal, VMware, AWS, Azure, or GCP).\n👉 Download the Installer\n3. Pull Secret The pull secret is required to access Red Hat’s container registries during installation.\n👉 Download Pull Secret\n4. DNS and Networking Ensure DNS resolution is properly configured for the cluster. Required DNS records: api.\u003ccluster_name\u003e.\u003cbase_domain\u003e → Load balancer or bootstrap node *.apps.\u003ccluster_name\u003e.\u003cbase_domain\u003e → Application load balancer Configure reverse DNS lookup for all nodes. Ensure required firewall ports are open (API, ingress, node-to-node communication).\n👉 OpenShift Networking Requirements 5. Infrastructure (VMs or Bare Metal) Provision infrastructure based on your target platform:\nBare Metal Minimum of 3 master nodes (control plane) Worker nodes (based on workload requirements) Hardware (per node): Master: 4 vCPU, 16 GB RAM, 120 GB disk Worker: 4 vCPU, 8 GB RAM, 120 GB disk VMware vSphere vCenter access with admin privileges Pre-created VM templates or ability to create VMs Shared datastore and networking Public Clouds (AWS, Azure, GCP) Proper IAM roles and permissions Region and VPC/subnet prepared Load balancers enabled Storage services (EBS, Azure Disk, GCP Persistent Disk) available 6. Cloud CLI Tools AWS CLI 👉 Download AWS CLI\nAzure CLI 👉 Download Azure CLI\nGCP SDK 👉 Download Google Cloud SDK\nVMware CLI Tools ovftool for importing templates vSphere client access 7. Certificates and Security Configure TLS certificates (self-signed or custom CA if required). Ensure SSH key pair is generated for cluster node access.\n👉 Generate SSH Key 8. Storage Requirements Infrastructure storage for etcd and control plane. Persistent storage for applications (NFS, Ceph, GlusterFS, EBS, Azure Disk, GCP PD, VMware vSAN).\n👉 Storage Requirements 9. Operating System Requirements Supported OS for nodes: Red Hat Enterprise Linux CoreOS (RHCOS) – default Red Hat Enterprise Linux (RHEL) with prerequisites installed Ensure subscription is attached for RHEL nodes. 10. Additional Tools (Optional but Recommended) oc CLI (OpenShift client) 👉 Download OC CLI kubectl CLI (for Kubernetes-level operations) Infrastructure automation tools (Terraform, Ansible) for IaC deployments ✅ With these prerequisites completed, you are ready to begin the OpenShift installation on your target platform.",
    "description": "Essential prerequisites for installing OpenShift across different platforms (bare metal, VMware, AWS, Azure, GCP).",
    "tags": [],
    "title": "OpenShift Prerequisites",
    "uri": "/ocp/prerequisites/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Redhat Openshift ",
    "uri": "/ocp/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Prerequisites Follow Github Repo\nChange timezone to UTC\ntimedatectl sudo timedatectl set-timezone UTC\rcheck network device status nmcli device status nmcli connection show\rActivate nmcli connection up \u003cconnection name\u003e\rAdd Network profile if ip address is not taken nmcli con add type ethernet ifname ens160 con-name ens160\rBring the connection up nmcli con up ens160\rConfigure Local yum using ISO Mount Iso Image mount /dev/sr0 /mnt/\rCreate the yum repo cat \u003c\u003c EOF \u003e /etc/yum.repos.d/rhel8-local.repo [BaseOS] name=RHEL8 BaseOS baseurl=file:///mnt/BaseOS enabled=1 gpgcheck=0 [AppStream] name=RHEL8 AppStream baseurl=file:///mnt/AppStream enabled=1 gpgcheck=0 EOF",
    "description": "Prerequisites Follow Github Repo\nChange timezone to UTC\ntimedatectl sudo timedatectl set-timezone UTC\rcheck network device status nmcli device status nmcli connection show\rActivate nmcli connection up \u003cconnection name\u003e\rAdd Network profile if ip address is not taken nmcli con add type ethernet ifname ens160 con-name ens160\rBring the connection up nmcli con up ens160\rConfigure Local yum using ISO Mount Iso Image mount /dev/sr0 /mnt/\rCreate the yum repo cat \u003c\u003c EOF \u003e /etc/yum.repos.d/rhel8-local.repo [BaseOS] name=RHEL8 BaseOS baseurl=file:///mnt/BaseOS enabled=1 gpgcheck=0 [AppStream] name=RHEL8 AppStream baseurl=file:///mnt/AppStream enabled=1 gpgcheck=0 EOF",
    "tags": [],
    "title": "OCP: Day 01",
    "uri": "/ocp-pre/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "OpenShift Licensing Models: Plus, Essentials, and Core Offerings Red Hat offers multiple versions of OpenShift that come with varying levels of functionality and features. These are primarily OpenShift Container Platform and OpenShift Plus, which are bundled with additional features. Below is a breakdown of these offerings.\n1. OpenShift Container Platform (OCP) This is the core product and includes the essential components required to deploy and manage OpenShift clusters on-premises or in the cloud. It includes:\nCore features: Kubernetes, Docker, container orchestration, and Red Hat’s operational tools. Security and Compliance: Built-in security features like SELinux, integrated container scanning, and vulnerability management. Monitoring and Logging: Integrated solutions for monitoring, alerting, and logging (via Prometheus, Grafana, and the ELK stack). Support: Access to Red Hat’s 24/7 support with multiple levels (Basic, Standard, Premium). 2. OpenShift Plus OpenShift Plus bundles additional value-added features on top of the OpenShift Container Platform. It is designed for enterprises that need more extensive tools for scaling, automation, and integration.\nKey features include:\nRed Hat OpenShift Service Mesh: Built on Istio, it enables observability, traffic management, and security for microservices. Red Hat OpenShift GitOps: Automates application deployment and lifecycle management using Git as the source of truth. Red Hat OpenShift Pipelines: A Kubernetes-native CI/CD pipeline tool for automating software delivery. Red Hat OpenShift Virtualization: Extends OpenShift to manage virtual machines alongside containers. Red Hat OpenShift Data Science: A platform to develop and deploy AI/ML models with Kubernetes. OpenShift Plus is generally aimed at larger organizations that need enhanced scalability, security, or specialized services (like machine learning, deep observability, or multi-cluster management).\n3. OpenShift Essentials OpenShift Essentials is typically aimed at developers and small teams who need a lightweight, simplified version of OpenShift. It’s a streamlined and more affordable offering than the full OpenShift Container Platform.\nKey differences compared to the full version:\nSimpler Setup and Maintenance: Aimed at reducing the complexity of managing OpenShift for smaller teams. Basic CI/CD and GitOps: Focuses on core container orchestration without extensive added tools. Limited Scalability: Typically suitable for smaller deployments, but may not scale as easily as the full platform. Limited Support: May not include 24/7 support or enterprise-level security features depending on the specific package. 4. OpenShift Online This is the fully managed version of OpenShift available as a public cloud service. It’s an ideal option for small teams or individual developers who don’t want to manage the underlying infrastructure but still want to leverage OpenShift’s capabilities.\nIt’s more cost-effective and offers flexibility in scaling up or down based on usage. Does not include full enterprise-grade features like the OpenShift Container Platform but provides the core platform with a hosted, managed environment. Comparison of Key Offerings Feature OpenShift Container Platform OpenShift Plus OpenShift Essentials OpenShift Online Core OpenShift features ✔️ ✔️ ✔️ ✔️ CI/CD (Pipelines, GitOps) ✔️ ✔️ Limited Limited Service Mesh (Istio) ❌ ✔️ ❌ ❌ Virtualization Support ❌ ✔️ ❌ ❌ Machine Learning \u0026 Data Science ❌ ✔️ ❌ ❌ Advanced Monitoring \u0026 Logging ✔️ ✔️ Limited ✔️ Scaling and Customization High High Limited Medium Support (Enterprise-Level) ✔️ ✔️ Limited ✔️ (via Red Hat) Licensing Models for OpenShift Plus and Essentials For OpenShift Plus, pricing is based on entitlements:\nPer Core or Per Node for OCP. Additional per-user or per-cluster charges for features like Service Mesh, GitOps, or Data Science. For OpenShift Essentials, pricing may be lower and is typically based on a more straightforward model, with an emphasis on smaller-scale, cost-effective deployments.\nOpenShift Types of Clusters OpenShift offers several types of clusters designed for different use cases and environments. Below is an overview of the different types of OpenShift clusters:\n1. OpenShift Container Platform (OCP) Cluster On-Premises: This is a self-managed cluster that you deploy on your own infrastructure, whether on bare metal or virtualized environments like VMware. Hybrid Cloud: You can also manage OpenShift clusters across hybrid cloud environments (on-premises and cloud). Key Features: Full control over the cluster and its configurations, ideal for enterprises needing robust security, high availability, and integration with existing on-prem infrastructure. Use Cases: Large-scale enterprise applications, private cloud environments. 2. OpenShift Dedicated Cluster Managed Service: OpenShift Dedicated is a fully managed service that runs on Amazon Web Services (AWS) or Google Cloud Platform (GCP). Red Hat manages the cluster on your behalf, so you don’t have to manage the underlying infrastructure. Key Features: Fully managed by Red Hat, ideal for organizations that want OpenShift but prefer not to handle the operational complexity. Use Cases: Companies that want to offload infrastructure management but still require enterprise-grade OpenShift capabilities in the cloud. 3. OpenShift Online Cluster Public Cloud Service: OpenShift Online is a fully managed public OpenShift service that runs on the cloud. It is designed for developers who need to quickly get started with Kubernetes without managing the infrastructure. Key Features: Simplified setup, cost-effective, no need to worry about cluster maintenance or scaling. Available for both free and paid tiers. Use Cases: Small teams, individual developers, and startups that want to use OpenShift with minimal setup. 4. OpenShift Container Platform with Red Hat OpenShift Virtualization (ROV) Cluster Virtualization Support: This is a specialized OpenShift cluster that integrates Red Hat OpenShift Virtualization, allowing organizations to run VMs alongside containers in the same Kubernetes-based cluster. Key Features: Combines container and virtual machine workloads, providing a unified platform for managing both. Use Cases: Organizations looking to migrate from legacy applications running in VMs to containerized environments. 5. OpenShift Container Platform with Red Hat OpenShift Service Mesh Cluster Service Mesh Support: This OpenShift cluster is enhanced with Red Hat OpenShift Service Mesh (based on Istio), which provides observability, traffic management, and security for microservices. Key Features: Features like intelligent routing, monitoring, and traffic management for microservices architectures. Use Cases: Microservices-based applications with complex service interactions that require advanced networking, monitoring, and security. 6. OpenShift Cluster on Public Cloud (AWS, Azure, Google Cloud) Cloud-Hosted Clusters: OpenShift can also be deployed on public cloud platforms like AWS, Azure, and GCP using Red Hat OpenShift on AWS (ROSA), Red Hat OpenShift on Azure (ARO), and Red Hat OpenShift on GCP. Key Features: These cloud-hosted clusters offer tight integration with cloud services while allowing organizations to benefit from OpenShift’s Kubernetes-based orchestration. Use Cases: Organizations looking to leverage OpenShift in the public cloud for scalability, high availability, and integration with cloud-native services. 7. OpenShift with Red Hat OpenShift Data Science AI/ML Support: This cluster includes additional tools to support data science workflows such as AI/ML model training and deployment. Key Features: Includes tools for building and deploying machine learning models in a Kubernetes-native environment, such as TensorFlow, PyTorch, and other popular data science frameworks. Use Cases: Teams working on machine learning, data science, and artificial intelligence projects. 8. OpenShift with Red Hat OpenShift Advanced Cluster Management (ACM) Multi-Cluster Management: OpenShift ACM allows you to manage multiple OpenShift clusters across different environments (on-premises, public cloud, hybrid) from a single console. Key Features: Centralized management, policy enforcement, and lifecycle management for multiple OpenShift clusters. Use Cases: Organizations operating in multi-cluster or hybrid environments that need centralized management and governance. 9. OpenShift Container Platform with Red Hat OpenShift Pipelines Cluster CI/CD Support: A cluster enhanced with Red Hat OpenShift Pipelines, a Kubernetes-native CI/CD tool that allows you to automate the software delivery process. Key Features: Integration with Jenkins, GitLab, and other CI/CD tools, providing pipeline automation within the OpenShift environment. Use Cases: Continuous integration and deployment needs for cloud-native applications. 10. OpenShift Edge Cluster Edge Computing: This OpenShift deployment is intended for edge computing environments where computing resources are deployed close to where data is generated (e.g., IoT, retail, manufacturing). Key Features: Lightweight, optimized for edge environments, supports remote management of edge clusters. Use Cases: Internet of Things (IoT) applications, real-time processing at the edge of the network. Summary of OpenShift Cluster Types Cluster Type Deployment Model Features Use Cases OpenShift Container Platform (OCP) On-Premises, Hybrid Full control, scalability, high availability, security Large-scale enterprise applications, private cloud environments OpenShift Dedicated Managed (AWS/GCP) Fully managed by Red Hat, no infrastructure management Organizations preferring to offload infrastructure management OpenShift Online Managed (Public Cloud) Simplified setup, cost-effective, developer-focused Small teams, individual developers, startups OpenShift with Red Hat Virtualization (ROV) On-Premises Runs both VMs and containers, unifying workloads Organizations migrating from legacy VMs to containerized environments OpenShift with Service Mesh On-Premises, Cloud Advanced networking and security for microservices Microservices-based applications with complex service interactions OpenShift on Public Cloud (AWS, Azure, GCP) Cloud Cloud-native, scalable, integrates with cloud services Public cloud-native workloads and hybrid cloud deployments OpenShift with Data Science On-Premises, Cloud Supports machine learning and AI/ML workloads Data science, AI/ML, model training and deployment OpenShift with ACM (Advanced Cluster Management) Multi-Cluster Centralized management of multiple clusters Multi-cluster, hybrid cloud, and enterprise-wide management OpenShift with Pipelines On-Premises, Cloud CI/CD for cloud-native applications Automating software delivery and deployment OpenShift Edge Edge Computing Optimized for distributed and remote edge environments IoT, real-time data processing at the edge Each cluster type is designed to address specific operational requirements, and your choice will depend on your infrastructure, workloads, and operational model.",
    "description": "OpenShift Licensing Models: Plus, Essentials, and Core Offerings Red Hat offers multiple versions of OpenShift that come with varying levels of functionality and features. These are primarily OpenShift Container Platform and OpenShift Plus, which are bundled with additional features. Below is a breakdown of these offerings.\n1. OpenShift Container Platform (OCP) This is the core product and includes the essential components required to deploy and manage OpenShift clusters on-premises or in the cloud. It includes:",
    "tags": [],
    "title": "OpenShift Overview",
    "uri": "/openshift-overview/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "OpenShift Single Node Installation Guide (Agent-Based Assisted Installation) Prerequisites Virtualization Software (choose one): VMware Workstation VirtualBox Hyper-V VMware vSphere Virtual Machine Requirements: Minimum 8 vCPUs, 32 GB RAM, and 120 GB storage (SSD recommended). Developer Account on Red Hat: cloud.redhat.com Installation Steps Log in to cloud.redhat.com and navigate to Services.\nSelect OpenShift and click Create Cluster.\nFill in the required details.\nProvide your SSH public key and download the generated ISO image.\nCreate a new VM using the downloaded ISO image.\nAssign 8 vCPUs, 32 GB RAM, and 120 GB storage. Once the cluster is deployed, the details will be visible in the Red Hat Console.\nDownload the kubeconfig file and kubeadmin credentials.\nPost-Installation Tasks (You can include cluster configuration, storage setup, monitoring, and user management here.)\nOpenShift and Azure AD Integration Prerequisites Azure AD Tenant with required user groups. A running OpenShift cluster. Integration Steps 1. Register an Application in Azure AD Log in to the Azure Portal. Navigate to Azure Active Directory → App registrations → New registration. Enter a name (e.g., OpenShiftApp). Set the Redirect URI to the OpenShift callback URL:\nhttps://\u003copenshift-cluster\u003e/oauth2callback Click Register. 2. Configure Azure AD for OpenShift OAuth 2.1 Create a Client Secret\nIn the registered app, go to Certificates \u0026 Secrets → New client secret. Add a description, choose an expiration period, and click Add. Copy and securely store the secret value. 2.2 Assign API Permissions\nNavigate to API permissions → Add a permission. Select Microsoft Graph → Delegated permissions. Add User.Read and any other required permissions. 3. Configure OpenShift OAuth for Azure AD On the OpenShift master node, edit the OAuth configuration:\napiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster annotations: release.openshift.io/create-only: \"true\" spec: identityProviders: - name: azuread mappingMethod: claim type: OpenID openID: clientID: \u003capplication-id\u003e clientSecret: name: azuread issuer: https://login.microsoftonline.com/\u003ctenant-id\u003e/v2.0 claims: email: - email id: - sub name: - name preferredUsername: - email",
    "description": "Step-by-step guide for deploying and integrating OpenShift Single Node clusters.",
    "tags": [],
    "title": "OpenShift Single Node Cluster",
    "uri": "/ocp/ocp-sno/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Openshift Alternatives",
    "uri": "/openshift-alternatives/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "Description This document describes the installation of OCP4 on the VMWare infra hosted on premise.\nRBAC required to manage the installation Be sure to be able to access to :\nthe dedicated vCenter https://vcenter01.local.com the bastion helper.local.com Informations about DNS The openshift will be reachable from Internet and from the LAN. It will use a public DNS record.\nA subzone of local.com. named ocp.local.com delgated to our team and is managed in an Azure DNS zone to give some freedom and responsivity when we need to manage it.\nIssues known During the installation of the production env, we were hit by the following bug : https://bugzilla.redhat.com/show_bug.cgi?id=1882022 .\nTo figure out we applied the following solution: https://access.redhat.com/solutions/5507161\nInformations required by the installer for the PRODUCTION key value basedomain local.com metadata.name ocp4 compute.hyperthreading Enabled compute.name worker compute.replicas 3 compute.platform.vsphere.cpus 8 compute.platform.vsphere.coresPerSocket 1 compute.platform.vsphere.memoryMB 16384 compute.platform.vsphere.osDisk.diskSizeGB 120 controlPlane.hyperthreading Enabled controlPlane.name master controlPlane.replicas 3 controlPlane.platform.vsphere.cpus 8 controlPlane.platform.vsphere.coresPerSocket 1 controlPlane.platform.vsphere.memoryMB 16384 controlPlane.platform.vsphere.osDisk.diskSizeGB 120 platform.vsphere.vcenter vcenter01.local.com platform.vsphere.username local.com\\administrator platform.vsphere.password find it in the secure place platform.vsphere.datacenter platform.vsphere.folder platform.vsphere.defaultDatastore ocp platform.vsphere.network ocp platform.vsphere.cluster ocp platform.vsphere.api_vip 192.168.1.100 platform.vsphere.ingress_vip 192.168.1.101 fips false pullSecret find it by going on https://cloud.redhat.com/openshift/create, navigate to the “DataCenter” tab, select “vsphere”, and “copy your pull secret sshKey '' networking.clusterNetwork.cidr 172.20.0.0/14 networking.clusterNetwork.hostPrefix 23 networking.clusterNetwork.cidr 172.20.0.0/14 networking.clusterNetwork.hostPrefix 23 networking.serviceNetwork 172.19.0.0/16 Example of install-config.yaml apiVersion: v1 baseDomain: local.com compute: - hyperthreading: Enabled name: worker replicas: 3 platform: vsphere: cpus: 8 coresPerSocket: 1 memoryMB: 16384 osDisk: diskSizeGB: 120 controlPlane: hyperthreading: Enabled name: master replicas: 3 platform: vsphere: cpus: 8 coresPerSocket: 1 memoryMB: 16384 osDisk: diskSizeGB: 120 metadata: name: ocp4 platform: vsphere: vcenter: vcenter01.local.com username: password: XXXXXXXXXXXXXXXXXXXXXXX datacenter: folder: defaultDatastore: network: cluster: apiVIP: ingressVIP: fips: false pullSecret: '{\"auths\":XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}}}' sshKey: networking: clusterNetwork: - cidr: 172.20.0.0/14 hostPrefix: 23 serviceNetwork: - 172.19.0.0/16\rThe VCenter certificate must be trusted (once !): curl -kSL https://vcenter01.local.com/certs/download.zip -o download.zip unzip download.zip sudo cp certs/lin/* /etc/pki/ca-trust/source/anchors sudo update-ca-trust extract\rPrerequisites have trusted the certificates on the bastion create an install-config.yaml file respecting the values described above In the VCenter, manually create the Folder that will be used by the installer.\nFor example, for creating the production env folder, go on the VMs and templates Reserve 2 static IPs that will be used by the ingress and the API create the matching Resource Records in the Azure DNS zone Create advanced network configuration Connect to the bastion ssh root@\u003cbastion\u003e\rCreate a working directory mkdir -p vsphere/$(date -I) cd vsphere\rcopy the install-config.yaml file you created in the working directory cp install-config.yaml $(date -I)/ tolerations: - effect: NoSchedule key: node-role.kubernetes.io/infra operator: Exists\rdownload the installer program. You can find all archived 4 versions under the following URL: https://mirror.openshift.com/pub/openshift-v4/clients/ocp\nWhile writting this doc, the version we deployed in other production env is the 4.6.18. vsphere $ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.6.18/openshift-install-linux-4.6.18.tar.gz vsphere $ tar xzvf openshift-install-linux-4.6.18.tar.gz\rcreate the manifests file: ./openshift-install create manifests --dir=$(date -I)/\rCreate a file that is named cluster-network-03-config.yml touch $(date -I)/manifests/cluster-network-03-config.yml Add the following content in this file apiVersion: operator.openshift.io/v1 kind: Network metadata: name: cluster spec: clusterNetwork: - cidr: 172.20.0.0/14 hostPrefix: 23 serviceNetwork: - 172.19.0.0/16 defaultNetwork: type: OVNKubernetes ovnKubernetesConfig: mtu: 1400 genevePort: 6081 Run the installer from the same working directory you created during the advanced network config step, run the openshift install tool ./openshift-install create cluster --dir=./$(date -I) --log-level=info\rIn production env it took around 40 minutes to be run:\nINFO Creating infrastructure resources... INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp4.local.com:6443... INFO API v1.19.0+f173eb4 up INFO Waiting up to 30m0s for bootstrapping to complete... INFO Destroying the bootstrap resources... INFO Waiting up to 40m0s for the cluster at https://api.ocp4.local.com:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/ocpadmin/vsphere/2021-03-25/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.local.com INFO Login to the console with user: \"kubeadmin\", and password: \"XXXXXXXXXXXX\" INFO Time elapsed: 38m24s\rinstall the oc client on the bastion (once) download and install oc curl https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.6.18/openshift-client-linux-4.6.18.tar.gz -o openshift-client-linux-4.6.18.tar.gz tar xzvf openshift-client-linux-4.6.18.tar.gz sudo cp oc /usr/local/bin/ sudo cp kubectl /usr/local/bin/\rManage the auto completion sudo yum install -y bash-completion oc completion bash \u003e ~/.kube/completion.bash.inc printf \" # Kubectl shell completion source '$HOME/.kube/completion.bash.inc' \" \u003e\u003e $HOME/.bash_profile source $HOME/.bash_profile\rRegistry installation temporarily decrease the registry replicas to 1 oc patch config.imageregistry.operator.openshift.io/cluster --type=merge -p '{\"spec\":{\"rolloutStrategy\":\"Recreate\",\"replicas\":1}}'\rDefine a PVC of 100Gi in a ‘pvc.yaml’ file kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-storage spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi create it in the right namespace oc create -f pvc.yaml -n openshift-image-registry edit the registry configuration oc edit config.imageregistry.operator.openshift.io -o yaml\rand modify the spec.storage configuration to match the following settings: storage: pvc: claim: image-registry-storage Wait for the PV has been successfully created and make the registry managed oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}'\rWait for the registry becomes ready watch oc get po -l docker-registry=default Check the cluster operator became ready in a non-degraded state oc get co image-registry\rTest the registry External references",
    "description": "Comprehensive OpenShift guides and documentation.",
    "tags": [],
    "title": "Openshift Installation on Vmware",
    "uri": "/ocp/ocp-onvmware-copy/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "Red Hat Openshift 4 Bare Metal OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 12: Create other OpenShift Users Step 13: Configure storage for the Image Registry Step 14: Troubleshooting Installation Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "description": "Red Hat Openshift 4 Bare Metal OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 12: Create other OpenShift Users Step 13: Configure storage for the Image Registry Step 14: Troubleshooting Installation Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "tags": [],
    "title": "Openshift Baremetal",
    "uri": "/ocp/openshift-baremetal.md/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "Table of Contents Pre Assessment - 20 Questions Introduction to OpenShift Overview of OpenShift: A comprehensive platform for developing, deploying, and managing containerized applications. Benefits of Using OpenShift: Scalability, flexibility, and ease of integration with CI/CD pipelines. Setting Up Your Environment Prerequisites: System requirements and necessary software. Installing OpenShift CLI (oc): Step-by-step guide for installation. Configuring Local Development Environments CodeReady Containers (CRC) What is CRC?: A tool for running OpenShift locally. Setting Up CRC Locally: Installation and configuration. Managing CRC Environments: Commands and best practices. Accessing the OpenShift Web Console: Navigating the UI for managing applications. Understanding OpenShift Architecture Key Components: Overview of Pods, Services, and Routes. OpenShift vs. Kubernetes: Differences and use cases. Creating and Managing Applications Building Applications: Development workflows in OpenShift. Source-to-Image (S2I) Process: Automating image creation from source code. Deploying Applications on OpenShift: Methods for deploying applications. Working with Containers Understanding Containers and Images: Core concepts and terminology. Managing Image Streams: Working with image repositories. Using Dockerfiles and Build Configurations: Customizing builds and images. Application Scaling and Management Scaling Applications: Manual and automatic scaling techniques. Rolling Updates and Rollbacks: Strategies for updating applications safely. Health Checks and Readiness Probes: Ensuring application reliability. Autoscaling Applications: Implementing horizontal and vertical scaling. Security in OpenShift Role-Based Access Control (RBAC): Managing user permissions and roles. Securing Applications and Data: Best practices for data security. Network Security Practices: Protecting application communication. Security Contexts and Policies: Configuring security for pods. Service Account: Managing service accounts for security. Networking in OpenShift Services and Routes: Exposing applications to the network. Configuring Ingress: Managing external access to services. Network Policies: Controlling traffic between pods. Persistent Storage in OpenShift Working with Persistent Volumes (PVs) and Persistent Volume Claims (PVCs): Managing storage. Storage Classes: Defining different types of storage. Monitoring and Logging Accessing Application Logs: Viewing logs for debugging. Integrating with Monitoring Tools: Tools for performance monitoring. Setting Up Alerts: Configuring alerts for important events. Metrics Collection and Visualization: Tools and techniques for metrics. CI/CD Integration Setting Up Pipelines using Jenkins: Automating the build and deployment process. Best Practices for Development Application Design Patterns: Effective design strategies. Managing Configurations: Keeping configurations organized. Troubleshooting Applications Common Issues and Solutions: Identifying and resolving frequent problems. Debugging Techniques: Tools and methods for troubleshooting. Managing Configuration Data Secrets and ConfigMaps Creating and Using Secrets: Storing sensitive information securely. Creating and Using ConfigMaps: Managing non-sensitive configuration data. Best Practices for Managing Sensitive Data: Ensuring security. Operators in OpenShift What are Operators?: Automating application management. Managing Operators with OperatorHub: Installing and configuring operators. Scheduling Options Understanding Scheduling in OpenShift: Basics of pod scheduling. Affinity and Anti-Affinity Node Affinity: Specifying node preferences. Pod Affinity: Co-locating pods based on criteria. Pod Anti-Affinity: Avoiding pod co-location. Taints and Tolerations What are Taints and Tolerations?: Managing node availability. Configuring Taints and Tolerations: Best practices. Using NodeSelectors: Directing pods to specific nodes. Scheduling with CronJobs Creating CronJobs for Scheduled Tasks: Automating recurring tasks. Post Assesment: 30 Questions",
    "description": "OpenShift for Developers",
    "tags": [],
    "title": "OpenShift for Developers",
    "uri": "/ocp/openshift-developer/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "Extending default monitoring stack in openshift 4 https://access.redhat.com/solutions/5298651\rHow to list internal registry repositories and images in OpenShift 4 https://access.redhat.com/solutions/5177301\rHow to generate a sosreport within nodes without SSH in OCP 4 https://access.redhat.com/solutions/4387261\rMonitoring operator is degraded with message “updates to statefulset spec for fields are forbidden https://access.redhat.com/solutions/6529271\rPreparing to upgrade to OpenShift Container Platform 4.9 https://access.redhat.com/articles/6329921\rFailing to create pod sandbox on OpenShift 3 and 4 https://access.redhat.com/solutions/4321791\rConsolidated Troubleshooting Article OpenShift Container Platform 4.x https://access.redhat.com/articles/4217411\rHow to rename identity provider in openshift https://access.redhat.com/solutions/3300361\rHow to re-create kubeconfig for system:admin user in Openshift 4 https://access.redhat.com/solutions/5286371\rBackend Performance Requirements for OpenShift etcd https://access.redhat.com/solutions/4770281\rHow to graph etcd metrics using Prometheus to gauge Etcd performance in OpenShift https://access.redhat.com/solutions/5489721\rHow to delete all kubernetes.io/events in etcd https://access.redhat.com/solutions/6171352\rHow to fix error on PVC https://devopstales.github.io/kubernetes/openshift-rbd-fsck/\rHow to update ssh keys for Openshift master nodes. https://access.redhat.com/solutions/3868301\rconfigure Alertmanager https://access.redhat.com/solutions/4882271",
    "description": "OpenShift Tasks",
    "tags": [],
    "title": "OpenShift Tasks",
    "uri": "/ocp/openshifttasks/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "OpenShift-customized-Training Prerequisites Aws/Azure Account Resources used 3 LoadBalancers Route 53 AWS s3 bucket 1 VPC 5 Ip addresses NAT gateways 8 VMS with 4 cpu and 8 GB of RAM each Duration:\nOpenShift Container Platform architecture\nOverview of Red Hat Enterprise Linux CoreOS (RHCOS)\nCrio OverView\nPodman Overview\nOverview of journactl\nHands on Lab\nCrio Podman Journalctl Installation and update\nUnderstand the underline infrstracture/resources requirements. Know Quay.io Know Redhat Registry know ignition files Installation with IPI Installation with user-provisioned infrastructure Installation on vmware(optional) Multi tenant Openshift Installation Configure Network Policy Installation with customized network plugins Troubleshooting installation issues Gathering logs from a failed installation Manually gathering logs with SSH access to your host(s) Manually gathering logs without SSH access to your host(s) Getting debug information from the installation program Post_installation_configuration This task will take most of the time\nConfiguration of Authentication with Htpasswd\nConfiguration of Authentication with Azure AD\nRemove the default virtual admin user (kubeadmin)\nSecure Api with ssl certificate\nSecure Route with Route\nSetting the Ingress Controller\nRestricting the API server to private\nConfiguration Default Quota project template\nConfigure default limits\nRestrict user for LoadBalancer service.\nConfigure Alert Manager\nUpdating the global cluster pull secret\nConfigure Autoscaling for nodes\nCreate infrastracture nodes\nMove all infr related services to infra nodes\nOpenShift Internal Registry\nRouter pods\nMonitoring pods\nLogging pods\nOpenShift Backup and DR\nInstallation and Configuration of Kasten/Velero Setup the backup of etcd Recovering from the etcd backup Post-installation node tasks\nAdding RHEL compute nodes to a cluster if needed Configuring Machine health checks Limitations when deploying machine health checks Node host best practices Configure different type of profile Updating ssh keys for master and worker nodes Post-installation network configuration\nEnabling the cluster-wide proxy Configuring ingress cluster traffic Configuring network policy Configuring multitenant isolation by using network policy Post-installation storage configuration\nDynamic provisioning Defining a storage class Using Azure file for RWX Installation and configuration cephcluster with rook operator to achieve below: Block storage File storage Object Storage Know OpenShift Internal Registry\nConfiguring additional trust stores for image registry access Configuring storage credentials for the Image Registry Operator OpenShift Scc\nUnderstanding default scc Creating and user custom scc Pod Scheduling*\nDefault scheduling Infrastructure Topological Levels Affinity Anti Affinity Advanced scheduling Pod Affinity and Anti-affinity Node Affinity Node Selectors Taints and Tolerations Custom scheduling Deploying the Scheduler Troubleshoot\nPod related issues\nRouter/Registry Not deploying to correct node Registry not showing contents of NFS mount (persistent volume) Hosts Can No Longer Resolve Each Other During Anisble Install Failure to deploy registry (permissions issues) Application Pod fails to deploy Issues with Nodes\nNodes being reported as ready, but builds failing Node reporting NotReady Nodes report ready but ETCD health check fails Atomic-openshift-node service fails to start Registry issues\nOpenShift builds fail trying to push image using a wrong IP address for the registry OpenShift build error: failed to push image while using NFS persistent storage Failure to push image to OpenShift’s Registry when backed by shared storage Quotas and Limitranges\nMust make a non-zero request for cpu Installation Fails…​\nWeb Console Public URL on a different Port UI Redirecting to the URL of the masters instead of the LB Intermittent Login issues (htpasswd) Build Issues oc new-app runs s2i instead of Docker build Binary Build Fails, citing “BadRequest” Issues related to Identity\nuser is unable to login user has two identities How to impersonate user login with service account Migration from Ocp3 to ocp 4",
    "description": "Openshift TOC",
    "tags": [],
    "title": "Openshift TOC",
    "uri": "/ocp/toc/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Red Hat Openshift 4 Bare Metal. OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 14: Troubleshooting Installation Step 12: Create other OpenShift Users Step 13:Configure storage for the Image Registry Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "description": "Red Hat Openshift 4 Bare Metal. OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 14: Troubleshooting Installation Step 12: Create other OpenShift Users Step 13:Configure storage for the Image Registry Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "tags": [],
    "title": "Openshift Baremetal",
    "uri": "/openshift-baremetal.md/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Table of Contents Pre Assessment - 20 Questions Introduction to OpenShift Overview of OpenShift: A comprehensive platform for developing, deploying, and managing containerized applications. Benefits of Using OpenShift: Scalability, flexibility, and ease of integration with CI/CD pipelines. Setting Up Your Environment Prerequisites: System requirements and necessary software. Installing OpenShift CLI (oc): Step-by-step guide for installation. Configuring Local Development Environments CodeReady Containers (CRC) What is CRC?: A tool for running OpenShift locally. Setting Up CRC Locally: Installation and configuration. Managing CRC Environments: Commands and best practices. Accessing the OpenShift Web Console: Navigating the UI for managing applications. Understanding OpenShift Architecture Key Components: Overview of Pods, Services, and Routes. OpenShift vs. Kubernetes: Differences and use cases. Creating and Managing Applications Building Applications: Development workflows in OpenShift. Source-to-Image (S2I) Process: Automating image creation from source code. Deploying Applications on OpenShift: Methods for deploying applications. Working with Containers Understanding Containers and Images: Core concepts and terminology. Managing Image Streams: Working with image repositories. Using Dockerfiles and Build Configurations: Customizing builds and images. Application Scaling and Management Scaling Applications: Manual and automatic scaling techniques. Rolling Updates and Rollbacks: Strategies for updating applications safely. Health Checks and Readiness Probes: Ensuring application reliability. Autoscaling Applications: Implementing horizontal and vertical scaling. Security in OpenShift Role-Based Access Control (RBAC): Managing user permissions and roles. Securing Applications and Data: Best practices for data security. Network Security Practices: Protecting application communication. Security Contexts and Policies: Configuring security for pods. Service Account: Managing service accounts for security. Networking in OpenShift Services and Routes: Exposing applications to the network. Configuring Ingress: Managing external access to services. Network Policies: Controlling traffic between pods. Persistent Storage in OpenShift Working with Persistent Volumes (PVs) and Persistent Volume Claims (PVCs): Managing storage. Storage Classes: Defining different types of storage. Monitoring and Logging Accessing Application Logs: Viewing logs for debugging. Integrating with Monitoring Tools: Tools for performance monitoring. Setting Up Alerts: Configuring alerts for important events. Metrics Collection and Visualization: Tools and techniques for metrics. CI/CD Integration Setting Up Pipelines using Jenkins: Automating the build and deployment process. Best Practices for Development Application Design Patterns: Effective design strategies. Managing Configurations: Keeping configurations organized. Troubleshooting Applications Common Issues and Solutions: Identifying and resolving frequent problems. Debugging Techniques: Tools and methods for troubleshooting. Managing Configuration Data Secrets and ConfigMaps Creating and Using Secrets: Storing sensitive information securely. Creating and Using ConfigMaps: Managing non-sensitive configuration data. Best Practices for Managing Sensitive Data: Ensuring security. Operators in OpenShift What are Operators?: Automating application management. Managing Operators with OperatorHub: Installing and configuring operators. Scheduling Options Understanding Scheduling in OpenShift: Basics of pod scheduling. Affinity and Anti-Affinity Node Affinity: Specifying node preferences. Pod Affinity: Co-locating pods based on criteria. Pod Anti-Affinity: Avoiding pod co-location. Taints and Tolerations What are Taints and Tolerations?: Managing node availability. Configuring Taints and Tolerations: Best practices. Using NodeSelectors: Directing pods to specific nodes. Scheduling with CronJobs Creating CronJobs for Scheduled Tasks: Automating recurring tasks. Post Assesment: 30 Questions",
    "description": "OpenShift for Developers",
    "tags": [],
    "title": "OpenShift for Developers",
    "uri": "/openshift-developer/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Openshift Cluster Installation ,Upgradation and Management. Prequisites: Login to AWS Cloud Create a Virtual Machine\nCreate a admin role and assign to the ec2 instance.\nCreate your account on Cloud.redhat.com Download Installer and Pull secret Follow the Redhat document for Installation\nConfigure AWS Account using link\nGet Domain from Freenom.com\nKnow AWS account Limit Create one Ec2 instance\nConfigure AWS Cli with Admin access on newly created ec2 instance.\nMake a test using below command\naws s3 ls\rList the OpenShift Container Platform web console route:\noc get routes -n openshift-console | grep 'console-openshift'\rLogin to Opeshift Cluster. oc login -u \u003cusername\u003e -p \u003cPassword\u003e\rLogin to Openshift using kubeconfig Login to OpenShift using Token oc login --token=\u003ctoken\u003e --server=\u003cCLuster URL\u003e\rVerifying the Health of OpenShift Nodes oc get nodes\rCheck the cpu and memory usage of each node oc adm top nodes\rCheck more information about a node oc describe node my-node-name\rCheck Openshift cluster Version oc get clusterversion oc describe clusterversion\rCheck Cluster Operator oc get co\rDisplaying the Logs of OpenShift Nodes oc adm node-logs -u crio my-node-name\rCheck the logs for kubelet Service oc adm node-logs -u kubelet my-node-name\rcheck all type of logs for a node oc adm node-logs my-node-name\rLogin to Cluster node using below command and check kubelet service oc debug node/my-node-name systemctl is-active kubelet\rDisplay the currently authenticated user oc whoami\rCheck the openshift URL oc whoami --show-console\rCheck openshift Api using below command oc whoami --show-server Check the token used by user oc whoami --show-token\rDebugging Openshift nodes with crictl https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/\nOpenshift 4 installation on AWS Description This document describes the installation of OCP4 on the AWS\nRBAC required to manage the installation Be sure to be able to access to :\nthe dedicated AWS environment as Admin for that cluster. the Redhat Account. to Azure account as Admin to create service Principles. Bastion environment. Informations about DNS There should be existing Public Hosted Zone in Route53. In case of stratus, we were having “stratus.soprasteria.com” as Hosted Zone.\nSteps To be Performed Prior to Installation Create a SSH key\nFor production OpenShift Container Platform clusters on which you want to perform installation debugging or disaster recovery, you must provide an SSH key that your ssh-agent process uses to the installer. You can use this key to access the bootstrap machine in a public cluster to troubleshoot installation issues.\nSteps to Generate SSH Key Generate the ssh-key through below command. ssh-keygen\rStart the ssh-agent process as a background task eval \"$(ssh-agent -s)\"\rAdd SSH private key to the ssh-agent ssh-add /root/.ssh/id_rsa\rExample of install-config.yaml [root@ip-172-31-32-217 ec2-user]# ssh-keygen\rGenerating public/private rsa key pair.\rEnter file in which to save the key (/root/.ssh/id_rsa):\rEnter passphrase (empty for no passphrase):\rEnter same passphrase again:\rYour identification has been saved in /root/.ssh/id_rsa.\rYour public key has been saved in /root/.ssh/id_rsa.pub.\rThe key fingerprint is:\rSHA256:vr+aWqjvwNJCkqS7ZSdZWzJjt0N1uSAqukjilRd4I/I root@ip-172-31-32-217.eu-west-3.compute.internal\rThe key's randomart image is:\r+---[RSA 3072]----+\r| |\r| . |\r| . . o o |\r|o. .. o o . |\r|+.ooB++ S . |\r| =oB+Oo+ |\r|+.BE*.+ o |\r|==.=.o o o |\r|+. .o+.+oo. |\r+----[SHA256]-----+\r[root@ip-172-31-32-217 ec2-user]# eval \"$(ssh-agent -s)\"\rAgent pid 13802\r[root@ip-172-31-32-217 ec2-user]# ssh-add /root/.ssh/id_rsa\rIdentity added: /root/.ssh/id_rsa (root@ip-172-31-32-217.eu-west-3.compute.internal)\rObtaining the installation program Access the Infrastructure Provider page on the OpenShift Cluster Manager site and Login to the Redhat Account. Select your infrastructure provider. Navigate to the page for your installation type, download the installation program for your operating system, and place the file in the directory where you will store the installation configuration files. wget https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/stable/openshift-install-linux.tar.gz\rExtract the installation program tar xvf openshift-install-linux.tar.gz\rAlso download the pull secret from the Red Hat OpenShift Cluster Manager. Deploying the cluster Login to the Bastion server.\nRun the below command to generate the Installation Config file.\n./openshift-install create install-config --dir install-new --log-level=debug\rNOTE :- As soon as you run the above command, it will ask for basic information like you need to select the Hosted zone, name of the cluster, Zone where you want your VM’s to run, ssh key and Pull secret.\nThis will create the Install config file. Example for the Installation Config file shown below.\napiVersion: v1 data: install-config: | apiVersion: v1 baseDomain: \u003cdomian\u003e compute: - architecture: amd64 hyperthreading: Enabled name: worker platform: {} replicas: 3 controlPlane: architecture: amd64 hyperthreading: Enabled name: master platform: {} replicas: 3 metadata: creationTimestamp: null name: newname networking: clusterNetwork: - cidr: 10.128.0.0/14 hostPrefix: 23 machineNetwork: - cidr: 10.0.0.0/16 networkType: OpenShiftSDN serviceNetwork: - 172.30.0.0/16 platform: aws: region: eu-west-3 publish: External pullSecret: \"\" sshKey: | ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDeRsMvsXUxzVsnn70zziGkmoCMGwS7A7gswhlvcCoswq1/q3j9yrh91Q8gF0aiKFJmsm45uD1r1DrXc10v+61fmdWcf862YldDBEKF34VN+SJT0wYEy8y3zso8hJ0csi2IB49ywXdw9OZaNk3UrCPbgVhhVZgP4g/thPGNjpJsI10ksuMIE+0Z4cQUbNPR8D1UbOrzPe45ziE1eKYsrRLj0tnQ2Zf8dBRF1NbnmpsfThoQhsQ8e/MGPIaD61BgRr6a/iFHDLYtn1TqyRaUqBrGM1r5hX4Kh6+zI8J1cPmgtNcJhoKvY38npkuKoJYqfhcuvxgKC7k3F43haTwR+5Sz ec2-user@ip-172-31-45-81.eu-west-3.compute.internal NOTE :- Verify the Install config file to check if you have the desired cluster configuration.\nNow run the below command to start the Cluster creation Process ./openshift-install create cluster --dir \u003cdirectory where install-config is present\u003e --log-level=debug\rNow, wait for the around 40-50 mins for the setup of the cluster where you will get the output similar to below one which will have kube-admin details as well as API cluster link.\n...\rINFO Install complete!\rINFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/myuser/install_dir/auth/kubeconfig'\rINFO Access the OpenShift web-console here: https://console-openshift-console.apps.mycluster.example.com\rINFO Login to the console with user: \"kubeadmin\", and password: \"4vYBz-Ee6gm-ymBZj-Wt5AL\"\rINFO Time elapsed: 36m22s\rImportant Links Openshift on AWS Installation Link. https://docs.openshift.com/container-platform/4.7/installing/installing_aws/installing-aws-default.html\nInstallation Program Link. https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/\novnkube resource containers from the nodes (Optional)\nfor i in $(oc get node | grep master | awk {'print $1'}); do echo $i; oc debug node/${i} -- chroot /host sh -c \"mkdir /tmp/ovnlogs \u0026\u0026 cp /var/log/containers/ovnkube*.log /tmp/ovnlogs\"; done\rTest Api server if any issue occurs oc exec -it apiserver-xxx -c openshift-apiserver -n openshift-apiserver sh curl -v 10.131.34.32:8443\rSimultaneously we collected tcpdump on all master nodes: $ tcpdump -nn -i any -w /tmp/$(hostname)-$(date +\"%Y-%m-%d-%H-%M-%S\").pcap\rif Api server is not reachanble from outside ,login to one of the master node and try below command curl -k https://{IP}:8443/readyz` to all openshift-apiserver pods\rHow to collect the inspect information for all co https://access.redhat.com/solutions/6995290\roc adm inspect co --request-timeout=30m\rCollect the inspect report for a single project oc adm ns inspect ns/openshift-apiserver\ryou get the below message once the installation is completed. INFO export KUBECONFIG=/home/ec2-user/auth/kubeconfig INFO Access the OpenShift web-console here: https://console-openshift-console.apps.myapp.openshift.gq INFO Login to the console with user: \"kubeadmin\", and password: \"479aZ-Z2QZy-UdCQy-9d9VN\"\rIf you have lost all the password like kubeadmin password and kubeconfig file as well you can still run the command. cd /etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/ export KUBECONFIG=$(pwd)/localhost.kubeconfig oc whoami\rCrictl (Command line tool for managing cri-o based containers and images) pull ubuntu image sudo crictl pull ubuntu\rList Images sudo crictl images\rDelete an image sudo crictl rmi 08d22c0ceb150\rPrint information about specific containers crictl inspect [container_id1 container_id2 ...]\rOpen a specific shell inside a running container crictl exec -it [container_id] [sh]\rPull a specific image from a registry crictl pull [image:tag]\rPrint and [f]ollow logs of a specific container crictl logs -f [container_id]\rRemove one or more images crictl rmi [image_id1 image_id2 ...]",
    "description": "Openshift Cluster Installation ,Upgradation and Management. Prequisites: Login to AWS Cloud Create a Virtual Machine\nCreate a admin role and assign to the ec2 instance.\nCreate your account on Cloud.redhat.com Download Installer and Pull secret Follow the Redhat document for Installation\nConfigure AWS Account using link\nGet Domain from Freenom.com\nKnow AWS account Limit Create one Ec2 instance\nConfigure AWS Cli with Admin access on newly created ec2 instance.",
    "tags": [],
    "title": "Openshift Installation and Upgradation",
    "uri": "/ocpsetup/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Single Node Openshift Prequisites: Download and Install Vmware workstation\nCreate a Virtual Machine with 100GB disk , 8CPU and 16GB of RAM\nCreate your account on Cloud.redhat.com Download Installer and Pull secret",
    "description": "Single Node Openshift Prequisites: Download and Install Vmware workstation\nCreate a Virtual Machine with 100GB disk , 8CPU and 16GB of RAM\nCreate your account on Cloud.redhat.com Download Installer and Pull secret",
    "tags": [],
    "title": "OpenShift on VMware Workstation",
    "uri": "/openshift-on-single-node/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Prerequisites Create Redhat Account Click Here to create Redhat Account Download the installer from below url Click Here to download the installer\nDownload pull secret to pull the image Click Here to download pull secret Create a VM Configure AWS cli if Installing on AWS cloud Click here to download awscli\nConfigure AWS cli if Installing on Azure cloud Click here to download azure cli",
    "description": "OpenShift Prerequisites",
    "tags": [],
    "title": "OpenShift Prerequisites",
    "uri": "/prerequisites./index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "OpenShift Tasks Extending default monitoring stack in openshift 4 https://access.redhat.com/solutions/5298651\rHow to list internal registry repositories and images in OpenShift 4 https://access.redhat.com/solutions/5177301\rHow to generate a sosreport within nodes without SSH in OCP 4 https://access.redhat.com/solutions/4387261\rMonitoring operator is degraded with message “updates to statefulset spec for fields are forbidden https://access.redhat.com/solutions/6529271\rPreparing to upgrade to OpenShift Container Platform 4.9 https://access.redhat.com/articles/6329921\rFailing to create pod sandbox on OpenShift 3 and 4 https://access.redhat.com/solutions/4321791\rConsolidated Troubleshooting Article OpenShift Container Platform 4.x https://access.redhat.com/articles/4217411\rHow to rename identity provider in openshift https://access.redhat.com/solutions/3300361\rHow to re-create kubeconfig for system:admin user in Openshift 4 https://access.redhat.com/solutions/5286371\rBackend Performance Requirements for OpenShift etcd https://access.redhat.com/solutions/4770281\rHow to graph etcd metrics using Prometheus to gauge Etcd performance in OpenShift https://access.redhat.com/solutions/5489721\rHow to delete all kubernetes.io/events in etcd https://access.redhat.com/solutions/6171352\rHow to fix error on PVC https://devopstales.github.io/kubernetes/openshift-rbd-fsck/\rHow to update ssh keys for Openshift master nodes. https://access.redhat.com/solutions/3868301\rconfigure Alertmanager https://access.redhat.com/solutions/4882271",
    "description": "OpenShift Tasks",
    "tags": [],
    "title": "OpenShift Tasks",
    "uri": "/openshifttasks/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "OpenShift-customized-Training Prerequisites Aws/Azure Account Resources used 3 LoadBalancers Route 53 AWS s3 bucket 1 VPC 5 Ip addresses NAT gateways 8 VMS with 4 cpu and 8 GB of RAM each Duration:\nOpenShift Container Platform architecture\nOverview of Red Hat Enterprise Linux CoreOS (RHCOS)\nCrio OverView\nPodman Overview\nOverview of journactl\nHands on Lab\nCrio Podman Journalctl Installation and update\nUnderstand the underline infrstracture/resources requirements. Know Quay.io Know Redhat Registry know ignition files Installation with IPI Installation with user-provisioned infrastructure Installation on vmware(optional) Multi tenant Openshift Installation Configure Network Policy Installation with customized network plugins Troubleshooting installation issues Gathering logs from a failed installation Manually gathering logs with SSH access to your host(s) Manually gathering logs without SSH access to your host(s) Getting debug information from the installation program Post_installation_configuration This task will take most of the time\nConfiguration of Authentication with Htpasswd\nConfiguration of Authentication with Azure AD\nRemove the default virtual admin user (kubeadmin)\nSecure Api with ssl certificate\nSecure Route with Route\nSetting the Ingress Controller\nRestricting the API server to private\nConfiguration Default Quota project template\nConfigure default limits\nRestrict user for LoadBalancer service.\nConfigure Alert Manager\nUpdating the global cluster pull secret\nConfigure Autoscaling for nodes\nCreate infrastracture nodes\nMove all infr related services to infra nodes\nOpenShift Internal Registry\nRouter pods\nMonitoring pods\nLogging pods\nOpenShift Backup and DR\nInstallation and Configuration of Kasten/Velero Setup the backup of etcd Recovering from the etcd backup Post-installation node tasks\nAdding RHEL compute nodes to a cluster if needed Configuring Machine health checks Limitations when deploying machine health checks Node host best practices Configure different type of profile Updating ssh keys for master and worker nodes Post-installation network configuration\nEnabling the cluster-wide proxy Configuring ingress cluster traffic Configuring network policy Configuring multitenant isolation by using network policy Post-installation storage configuration\nDynamic provisioning Defining a storage class Using Azure file for RWX Installation and configuration cephcluster with rook operator to achieve below: Block storage File storage Object Storage Know OpenShift Internal Registry\nConfiguring additional trust stores for image registry access Configuring storage credentials for the Image Registry Operator OpenShift Scc\nUnderstanding default scc Creating and user custom scc Pod Scheduling*\nDefault scheduling Infrastructure Topological Levels Affinity Anti Affinity Advanced scheduling Pod Affinity and Anti-affinity Node Affinity Node Selectors Taints and Tolerations Custom scheduling Deploying the Scheduler Troubleshoot\nPod related issues\nRouter/Registry Not deploying to correct node Registry not showing contents of NFS mount (persistent volume) Hosts Can No Longer Resolve Each Other During Anisble Install Failure to deploy registry (permissions issues) Application Pod fails to deploy Issues with Nodes\nNodes being reported as ready, but builds failing Node reporting NotReady Nodes report ready but ETCD health check fails Atomic-openshift-node service fails to start Registry issues\nOpenShift builds fail trying to push image using a wrong IP address for the registry OpenShift build error: failed to push image while using NFS persistent storage Failure to push image to OpenShift’s Registry when backed by shared storage Quotas and Limitranges\nMust make a non-zero request for cpu Installation Fails…​\nWeb Console Public URL on a different Port UI Redirecting to the URL of the masters instead of the LB Intermittent Login issues (htpasswd) Build Issues oc new-app runs s2i instead of Docker build Binary Build Fails, citing “BadRequest” Issues related to Identity\nuser is unable to login user has two identities How to impersonate user login with service account Migration from Ocp3 to ocp 4",
    "description": "Openshift TOC",
    "tags": [],
    "title": "Openshift TOC",
    "uri": "/toc/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Topics Describing Red Hat OpenShift Container Platform",
    "description": "Topics Describing Red Hat OpenShift Container Platform",
    "tags": [],
    "title": "Part 1",
    "uri": "/part-1/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "Openshift Authentication and Authorization Htpasswd Configuration: yum install httpd* htpasswd -c -B -b /tmp/htpasswd student redhat123 htpasswd -B -b /tmp/htpasswd student2 redhat123 oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config vi oauth.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldap mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpasswd-secret\rConplete this as well https://gitlab.com/gcpnirpendra/openshift-280-training/-/issues/7\rProject How to create a project oc new-project myapp\rCheck all the project you have got access to oc get projects\rCheck your current project oc project\rGive access to Project from UI How to create a pod oc run \u003cname\u003e --image=nginx\rAnother example vi pod.yanl\rapiVersion: v1\rkind: Pod\rmetadata:\rname: firstpod\rspec:\rcontainers:\r- name: firstcontainer\rimage: nginx\rHow to check all pods oc get pods\rHow to check all pods in all namespaces oc get pods -A\rHow to check the name of all pods oc get pods -o name\rHow to check the labels of all pods running oc get pods --show-lables\rHow to login to a pods oc exec -it podname sh How to check the logs of a pods oc logs podname\rHow to login to a pod oc exec -it podname sh\rHow to delete a pod oc delete pod \u003cpodname\u003e\rHow to delete a pod forcefully oc delete pod --force --grace-period=0\rHow to delete multiple pod in one go for i in $(oc get pods -o name ); do oc delete pod $i ;done How to check the logs for a specific container oc logs podname -c \u003ccontainer-name\u003e\rHow to login to specific container oc exec -it podname -c \u003ccontainer-name\u003e\rService How to check all services oc get svc\rHow to create a service oc expose pod/deployment deployment/myapp --port=80\rService with yaml manifest apiVersion: v1\rkind: Service\rmetadata:\rname: my-service\rspec:\rselector:\rapp: MyApp\rports:\r- protocol: TCP\rport: 80\rtargetPort: 9376\rHow to check the labels where the service is forwarding requests oc describe svc \u003csvcname\u003e\rHow to check the endpoints of a service oc get ep\rDeployments How to create a deployment oc create deployment myapp --image=nginx\rRolling update “www” containers of “frontend” deployment, updating the image oc set image deployment/frontend www=image:v2 Check the history of deployments including the revision oc rollout history deployment/frontend Rollback to the previous deployment oc rollout undo deployment/frontend Rollback to a specific revision oc rollout undo deployment/frontend --to-revision=2 Watch rolling update status of “frontend” deployment until completion oc rollout status -w deployment/frontend Rolling restart of the “frontend” deployment oc rollout restart deployment/frontend Secrets Check all secrets oc get secrets\rHow to create secret oc create secret tls my-tls-secret \\\r--cert=path/to/cert/file \\\r--key=path/to/key/file\rCreate Secret for username and password oc create secret generic creds --from-literal=name=nippy --from=literal=pass=123\rList the environment variables defined on all pods oc set env pods --all --list\rImport environment from a secret oc set env --from=secret/mysecret dc/myapp\rImport environment from a config map with a prefix oc set env --from=configmap/myconfigmap --prefix=MYSQL_ dc/myapp\rRemove the environment variable ENV from container ‘c1’ in all deployment configs oc set env deployments --all --containers=\"c1\" ENV-\rConfigMaps List all configmaps oc get cm\rCreate a new configmap oc create configmap game-config --from-file=configure-pod-container\rGet the yaml file for the configmap oc get configmaps game-config -o yaml\rUse configmap as a volume in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: dapi-test-pod\rspec:\rcontainers:\r- name: test-container\rimage: k8s.gcr.io/busybox\rcommand: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ]\rvolumeMounts:\r- name: config-volume\rmountPath: /etc/config\rvolumes:\r- name: config-volume\rconfigMap:\r# Provide the name of the ConfigMap containing the files you want\r# to add to the container\rname: special-config\rrestartPolicy: Never\rVolumes emptyDir Example apiVersion: v1\rkind: Pod\rmetadata:\rname: test-pd\rspec:\rcontainers:\r- image: registry.k8s.io/test-webserver\rname: test-container\rvolumeMounts:\r- mountPath: /cache\rname: cache-volume\rvolumes:\r- name: cache-volume\remptyDir:\rsizeLimit: 500Mi\rCheck all pvcs oc get pvc\rCreate a pvc apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv0001 spec:\rcapacity:\rstorage: 5Gi accessModes:\r- ReadWriteOnce use pvc inside a pod kind: Pod\rapiVersion: v1\rmetadata:\rname: mypod\rspec:\rcontainers:\r- name: myfrontend\rimage: dockerfile/nginx\rvolumeMounts:\r- mountPath: \"/var/www/html\" name: mypd volumes:\r- name: mypd\rpersistentVolumeClaim:\rclaimName: myclaim Kubernetes Trobleshooting Check the nodes status oc get nodes\rCheck the status of pods oc get pods Check the pods where are they scheduled oc get pod -o wide\rcheck the events for a particular namepsace oc get events\rLook for any error in pod oc logs podname\rDescribe pod if the pod status is pending oc describe pod podname\rHow to login to private image registry docker login\rCreate a secret for imagePull secret oc create secret generic regcred \\\r--from-file=.dockerconfigjson=\u003cpath/to/.docker/config.json\u003e \\\r--type=kubernetes.io/dockerconfigjson\rUse Imagepull secret in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: private-reg\rspec:\rcontainers:\r- name: private-reg-container\rimage: \u003cyour-private-image\u003e\rimagePullSecrets:\r- name: regcred\rCheck the ip address of pod with specifice label oc get pods -l app=hostnames \\\r-o go-template='{{range .items}}{{.status.podIP}}{{\"\\n\"}}{{end}}'\rTry to check the connectivity from a pod for ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; do\rwget -qO- $ep\rdone",
    "description": "Day02",
    "tags": [],
    "title": "Part 2",
    "uri": "/ocp/part-2/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Openshift Authentication and Authorization Htpasswd Configuration: yum install httpd* htpasswd -c -B -b /tmp/htpasswd student redhat123 htpasswd -B -b /tmp/htpasswd student2 redhat123 oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config vi oauth.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldap mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpasswd-secret\rConplete this as well https://gitlab.com/gcpnirpendra/openshift-280-training/-/issues/7\rProject How to create a project oc new-project myapp\rCheck all the project you have got access to oc get projects\rCheck your current project oc project\rGive access to Project from UI How to create a pod oc run \u003cname\u003e --image=nginx\rAnother example vi pod.yanl\rapiVersion: v1\rkind: Pod\rmetadata:\rname: firstpod\rspec:\rcontainers:\r- name: firstcontainer\rimage: nginx\rHow to check all pods oc get pods\rHow to check all pods in all namespaces oc get pods -A\rHow to check the name of all pods oc get pods -o name\rHow to check the labels of all pods running oc get pods --show-lables\rHow to login to a pods oc exec -it podname sh How to check the logs of a pods oc logs podname\rHow to login to a pod oc exec -it podname sh\rHow to delete a pod oc delete pod \u003cpodname\u003e\rHow to delete a pod forcefully oc delete pod --force --grace-period=0\rHow to delete multiple pod in one go for i in $(oc get pods -o name ); do oc delete pod $i ;done How to check the logs for a specific container oc logs podname -c \u003ccontainer-name\u003e\rHow to login to specific container oc exec -it podname -c \u003ccontainer-name\u003e\rService How to check all services oc get svc\rHow to create a service oc expose pod/deployment deployment/myapp --port=80\rService with yaml manifest apiVersion: v1\rkind: Service\rmetadata:\rname: my-service\rspec:\rselector:\rapp: MyApp\rports:\r- protocol: TCP\rport: 80\rtargetPort: 9376\rHow to check the labels where the service is forwarding requests oc describe svc \u003csvcname\u003e\rHow to check the endpoints of a service oc get ep\rDeployments How to create a deployment oc create deployment myapp --image=nginx\rRolling update “www” containers of “frontend” deployment, updating the image oc set image deployment/frontend www=image:v2 Check the history of deployments including the revision oc rollout history deployment/frontend Rollback to the previous deployment oc rollout undo deployment/frontend Rollback to a specific revision oc rollout undo deployment/frontend --to-revision=2 Watch rolling update status of “frontend” deployment until completion oc rollout status -w deployment/frontend Rolling restart of the “frontend” deployment oc rollout restart deployment/frontend Secrets Check all secrets oc get secrets\rHow to create secret oc create secret tls my-tls-secret \\\r--cert=path/to/cert/file \\\r--key=path/to/key/file\rCreate Secret for username and password oc create secret generic creds --from-literal=name=nippy --from=literal=pass=123\rList the environment variables defined on all pods oc set env pods --all --list\rImport environment from a secret oc set env --from=secret/mysecret dc/myapp\rImport environment from a config map with a prefix oc set env --from=configmap/myconfigmap --prefix=MYSQL_ dc/myapp\rRemove the environment variable ENV from container ‘c1’ in all deployment configs oc set env deployments --all --containers=\"c1\" ENV-\rConfigMaps List all configmaps oc get cm\rCreate a new configmap oc create configmap game-config --from-file=configure-pod-container\rGet the yaml file for the configmap oc get configmaps game-config -o yaml\rUse configmap as a volume in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: dapi-test-pod\rspec:\rcontainers:\r- name: test-container\rimage: k8s.gcr.io/busybox\rcommand: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ]\rvolumeMounts:\r- name: config-volume\rmountPath: /etc/config\rvolumes:\r- name: config-volume\rconfigMap:\r# Provide the name of the ConfigMap containing the files you want\r# to add to the container\rname: special-config\rrestartPolicy: Never\rVolumes emptyDir Example apiVersion: v1\rkind: Pod\rmetadata:\rname: test-pd\rspec:\rcontainers:\r- image: registry.k8s.io/test-webserver\rname: test-container\rvolumeMounts:\r- mountPath: /cache\rname: cache-volume\rvolumes:\r- name: cache-volume\remptyDir:\rsizeLimit: 500Mi\rCheck all pvcs oc get pvc\rCreate a pvc apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv0001 spec:\rcapacity:\rstorage: 5Gi accessModes:\r- ReadWriteOnce use pvc inside a pod kind: Pod\rapiVersion: v1\rmetadata:\rname: mypod\rspec:\rcontainers:\r- name: myfrontend\rimage: dockerfile/nginx\rvolumeMounts:\r- mountPath: \"/var/www/html\" name: mypd volumes:\r- name: mypd\rpersistentVolumeClaim:\rclaimName: myclaim Kubernetes Trobleshooting Check the nodes status oc get nodes\rCheck the status of pods oc get pods Check the pods where are they scheduled oc get pod -o wide\rcheck the events for a particular namepsace oc get events\rLook for any error in pod oc logs podname\rDescribe pod if the pod status is pending oc describe pod podname\rHow to login to private image registry docker login\rCreate a secret for imagePull secret oc create secret generic regcred \\\r--from-file=.dockerconfigjson=\u003cpath/to/.docker/config.json\u003e \\\r--type=kubernetes.io/dockerconfigjson\rUse Imagepull secret in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: private-reg\rspec:\rcontainers:\r- name: private-reg-container\rimage: \u003cyour-private-image\u003e\rimagePullSecrets:\r- name: regcred\rCheck the ip address of pod with specifice label oc get pods -l app=hostnames \\\r-o go-template='{{range .items}}{{.status.podIP}}{{\"\\n\"}}{{end}}'\rTry to check the connectivity from a pod for ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; do\rwget -qO- $ep\rdone",
    "description": "Day02",
    "tags": [],
    "title": "Part 2",
    "uri": "/part-2/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "Use and configure OpenID connect with Azure AD\nUse and configure OpenID connect with Azure AD Required informations key env value service principal staging “RedHat - Openshift / (auth)” () Azure AD service principal requirements Ensure the service principal has the following API permissions Microsoft Graph / \"User.Read\"\nEnsure it has been “Granted for, .\nEnsure it has been promoted as Enterprise Application, otherwise raise an internal IT request to get this requirement. It could allow to restrict the access to a group of users that is very useful.\nEnsure the redirect URI is properly set, it must match the following pattern: https://oauth-openshift.apps.\u003cfullclustername\u003e/oauth2callback/azuread. Of course substitute \u003cfullclustername\u003e by the one you set in the IPI configuration file.\nOpenshift settings Create an openshift secret\nbe sure to substitute the \u003capplication registration secret\u003e by the right value ! oc create secret generic azuread --from-literal=clientSecret=\u003capp registration secret\u003e -n openshift-config\rCreate an oauth-config.yaml file to configure oauth. be sure to substitute the \u003capplication id\u003e by the right value !\napiVersion: config.openshift.io/v1 kind: OAuth metadata: annotations: release.openshift.io/create-only: \"true\" generation: 1 name: cluster selfLink: /apis/config.openshift.io/v1/oauths/cluster spec: identityProviders: - mappingMethod: claim name: azuread openID: claims: email: - email id: - sub name: - name preferredUsername: - email clientID: \u003capplication id\u003e clientSecret: name: azuread issuer: https://login.microsoftonline.com/8b87af7d-8647-4dc7-8df4-5f69a2011bb5/v2.0 type: OpenID\rapply the config oc apply -f oauth-config.yaml\rPost configuration tests wait a moment and try to login in a new private window of your internet browser.\nazuread should be an option available and it must work properly. Post configuration steps signin as kubeadmin and promote your cloud admin account as cluster-admin Example:\noc adm policy add-cluster-role-to-user cluster-admin user-id\rlogout and login with your own admin account. Ensure you have the cluster-admin role and remove the kubeadmin secret oc delete secrets kubeadmin -n kube-system\rlogout and ensure the kube:admin option is not available anymore.\nif you need to restrict access to the cluster, go on the Azure AD, select your entreprise application, go in the “Users and Groups” blade, and select the ones who could access to the instance.\nPlease click to configure Azure Authentication from UI",
    "description": "Use and configure OpenID connect with Azure AD",
    "tags": [],
    "title": "Use and configure OpenID connect with Azure AD",
    "uri": "/openid-azuread/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "description": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "tags": [],
    "title": "Virtual Box",
    "uri": "/ocp/virtualbox/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "description": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "tags": [],
    "title": "Virtual Box",
    "uri": "/virtualbox/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e Redhat Openshift",
    "content": "OpenShift Administrators Roles and Responsibilities Updating SSH Keys on OpenShift 4 Nodes via MachineConfig Adding an SSL Certificate to Trusted Store in OpenShift Cluster SSL Related Tasks in OpenShift 4 Administration Updating the Default SSL Certificate Changing Configuration for All OpenShift Nodes Adjusting Kernel Parameters Changing the Default Container Runtime Settings Upgrading OpenShift Cluster Upgrading OpenShift Nodes in Batches How to change the no of node unavailable while upgrading Modifying OpenShift Project Templates Adding Resource Quotas Adding LimitRanges Exposing the Internal OpenShift Registry Overview of the openshift-config Namespace in OpenShift OpenShift Administrators Roles and Responsibilities. Capacity Planning\nProof of concepts\nCluster Installation and Upgrades\nSet up new clusters, apply regular updates, and perform major upgrades. This includes managing Red Hat OpenShift Cluster Manager and leveraging tools like Red Hat Advanced Cluster Management (RHACM) for multi-cluster environments. User Management and Access Control\nImplement role-based access control (RBAC) policies, manage users and permissions, and integrate authentication providers like LDAP or Active Directory. Storage Management\nConfigure persistent storage using OpenShift Container Storage (OCS) or other storage providers. Manage storage classes, Persistent Volumes (PVs), and Persistent Volume Claims (PVCs) to ensure reliable data storage for applications. Networking and Ingress Configuration\nManage networking policies, routes, ingress controllers, and network security configurations. Tasks include setting up cluster networking (SDN or OpenShift SDN), load balancing, and managing multi-cluster networking with RHACM. Monitoring and Logging\nUse Prometheus and Grafana for monitoring, and EFK/ELK stacks (Elasticsearch, Fluentd, Kibana) for centralized logging. Set up alerting for resource utilization, network traffic, and error logs to proactively manage cluster health. Security Compliance and Vulnerability Management\nApply security patches, manage OpenShift’s integrated security features, and scan images for vulnerabilities. Enable features like the OpenShift Compliance Operator for automated security checks. Resource Optimization and Quota Management\nMonitor and optimize resource usage (CPU, memory), and set quotas and limits for namespaces and projects to prevent resource exhaustion and ensure fair resource allocation. Backup and Disaster Recovery\nImplement backup and restore strategies using tools like Velero for Kubernetes backups, Kasten, or Red Hat-supported backup solutions. DR planning includes setting up automated backups and validating restore procedures regularly. Operator Lifecycle Management (OLM)\nInstall, upgrade, and manage Operators for automating infrastructure components and applications. Ensure Operators are kept up-to-date and compatible with the OpenShift environment. Support and Troubleshooting\nRegularly troubleshoot performance issues, manage logs, resolve configuration errors, and work with Red Hat Support or internal resources for escalations. Common issues include network latency, failed deployments, or resource constraints impacting applications. These tasks keep an OpenShift 4 environment secure, high-performing, and aligned with organizational requirements, especially for production-grade clusters.\nUpdating SSH Keys on OpenShift 4 Nodes via MachineConfig OpenShift 4 uses the Machine Config Operator (MCO) to manage node configurations. To update SSH keys across all master and worker nodes, create a MachineConfig resource to apply the new SSH public key.\nStep 1: Generate a New SSH Key Pair (Optional) If you don’t already have an SSH key pair, generate a new one on your local machine:\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\rThis command creates two files:\n~/.ssh/id_rsa (private key) ~/.ssh/id_rsa.pub (public key) Step 2: Create a MachineConfig YAML File for SSH Key Update Create a YAML file, e.g., ssh-key-update.yaml, with the following content. Replace \u003cyour-ssh-public-key\u003e with the actual content of your SSH public key.\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: ssh-key-update labels: machineconfiguration.openshift.io/role: \u003cnode-role\u003e # Replace \u003cnode-role\u003e with \"worker\" or \"master\" spec: config: ignition: version: 3.2.0 passwd: users: - name: core sshAuthorizedKeys: - \"ssh-rsa \u003cyour-ssh-public-key\u003e\"\rmachineconfiguration.openshift.io/role: Set this to worker to target worker nodes or master to target master nodes. sshAuthorizedKeys: Add your SSH public key here (the content of id_rsa.pub). Step 3: Apply the MachineConfig Apply the MachineConfig to your OCP 4 cluster: oc apply -f ssh-key-update.yaml\rMonitor the rollout status to confirm that the update is applied across nodes: oc get machineconfigpool\rWait until the UPDATED status is True for the respective machineconfigpool (e.g., worker or master).\nStep 4: Verify the SSH Key Update Once the rollout is complete, verify that you can SSH into the nodes as the core user:\nssh core@\u003cnode-ip\u003e\rThis should authenticate using the updated SSH key.\nImportant Notes:\nDo not manually modify SSH keys directly on nodes, as the Machine Config Operator will revert changes outside of its control. Role-based MachineConfig: If you need different keys for master and worker nodes, create separate MachineConfig files, each with the appropriate machineconfiguration.openshift.io/role label. Automation: Changes applied with MachineConfig will persist across reboots and node replacements, aligning with OpenShift’s immutable infrastructure approach. Adding an SSL Certificate to Trusted Store in OpenShift Cluster To add an SSL certificate to the trusted store in an OpenShift cluster, follow these steps:\nStep 1: Obtain the SSL Certificate Make sure you have the SSL certificate file (e.g., my-certificate.crt) that you want to add to the trusted store.\nStep 2: Create a ConfigMap or Secret You can either use a ConfigMap or a Secret to store the SSL certificate. A Secret is recommended for sensitive data.\nCreate a Secret Create a Secret containing the SSL certificate: oc create secret generic my-ssl-cert --from-file=ca.crt=my-certificate.crt -n \u003cnamespace\u003e\rReplace \u003cnamespace\u003e with the desired namespace where you want to create the secret.\nCreate a ConfigMap (Optional) If you choose to use a ConfigMap, you can create it with:\noc create configmap my-ssl-cert --from-file=my-certificate.crt -n \u003cnamespace\u003e\rStep 3: Configure the Trusted CA To configure the OpenShift cluster to trust the SSL certificate, you need to add it to the appropriate configuration.\nFor Custom Certificates Edit the ClusterIP service to use the custom CA: oc patch configmap/cluster --type merge -p '{\"data\":{\"trust-ca.crt\": \"your-certificate-data\"}}' -n openshift-config\rReplace \"your-certificate-data\" with the actual contents of your SSL certificate.\nStep 4: Update the API Server Edit the API server configuration to include your SSL certificate: oc patch apiserver cluster --type merge --patch '{\"spec\":{\"trustedCA\":{\"name\":\"my-ssl-cert\"}}}'\rStep 5: Restart Affected Pods To apply the changes, you may need to restart the affected pods, such as the API server or any applications that need to trust the new certificate.\nRestart the affected pods: oc rollout restart deployment/\u003cdeployment-name\u003e -n \u003cnamespace\u003e\rStep 6: Verify the Configuration Verify that the certificate has been added to the trusted store: You can check the trusted certificates by accessing a pod and verifying the CA store: oc exec -it \u003cpod-name\u003e -- cat /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\rImportant Notes:\nAlways ensure you have the correct permissions to create secrets and modify configurations in the cluster. Be cautious when handling sensitive information like SSL certificates. Depending on your OpenShift version and setup, some steps may vary slightly. SSL Related Tasks in OpenShift 4 Administration As an OpenShift 4 administrator, managing SSL certificates is crucial for securing communications. This guide outlines common SSL-related tasks, including adding custom certificates, renewing certificates, and managing the default certificate.\nTable of Contents Adding a Custom SSL Certificate Renewing SSL Certificates Updating the Default SSL Certificate Verifying SSL Certificates Adding a Custom SSL Certificate Step 1: Obtain the SSL Certificate Ensure you have the SSL certificate files (e.g., my-certificate.crt and my-key.key).\nStep 2: Create a Secret for the SSL Certificate Create a Secret containing the SSL certificate and key: oc create secret tls my-tls-secret --cert=my-certificate.crt --key=my-key.key -n \u003cnamespace\u003e\rReplace \u003cnamespace\u003e with the desired namespace.\nStep 3: Configure Ingress or Routes to Use the Custom SSL Certificate Edit your Ingress or Route to reference the Secret: apiVersion: route.openshift.io/v1 kind: Route metadata: name: my-route namespace: \u003cnamespace\u003e spec: host: myapp.example.com to: kind: Service name: my-service tls: termination: edge certificate: | -----BEGIN CERTIFICATE----- \u003cyour-certificate-content\u003e -----END CERTIFICATE----- key: | -----BEGIN PRIVATE KEY----- \u003cyour-key-content\u003e -----END PRIVATE KEY----- Renewing SSL Certificates Step 1: Obtain the New SSL Certificate Acquire the new SSL certificate files when it’s time to renew your certificate.\nStep 2: Update the Existing Secret Update the Secret with the new certificate and key: oc create secret tls my-tls-secret --cert=new-certificate.crt --key=new-key.key -n \u003cnamespace\u003e --dry-run=client -o yaml | oc apply -f -\rStep 3: Verify the Route Ensure that the updated SSL certificate is being used:\noc get route my-route -n \u003cnamespace\u003e -o yaml\rUpdating the Default SSL Certificate Step 1: Create or Update a ConfigMap If you need to update the default certificates for the cluster (e.g., for the API server), create or update a ConfigMap:\noc create configmap custom-ca --from-file=my-certificate.crt -n openshift-config --dry-run=client -o yaml | oc apply -f -\rStep 2: Patch the API Server Patch the API server to use the new custom CA: oc patch apiserver cluster --type merge --patch '{\"spec\":{\"trustedCA\":{\"name\":\"custom-ca\"}}}'\rVerifying SSL Certificates Step 1: Verify SSL Certificate in Use You can verify which SSL certificate is currently in use for a route:\noc get route my-route -n \u003cnamespace\u003e -o jsonpath='{.spec.tls}'\rStep 2: Check the Certificate Chain To check the certificate chain of a service, you can exec into a pod and use tools like openssl:\noc exec -it \u003cpod-name\u003e -- /bin/sh openssl s_client -connect myapp.example.com:443 -showcerts\rImportant Notes:\nEnsure you have the necessary permissions to manage secrets and configurations in OpenShift. Always back up existing certificates and configurations before making changes. Monitor the expiration of certificates and set reminders to renew them in advance. Changing Configuration for All OpenShift Nodes OpenShift 4 uses the Machine Config Operator to manage the configuration of nodes. To change the configuration for all nodes, follow these steps:\nStep 1: Identify the Configuration Change Determine what specific configuration you need to change. Common changes include:\nUpdating SSH keys Modifying systemd services Adjusting kernel parameters Changing the default container runtime settings Step 2: Create a MachineConfig Create a YAML file for the MachineConfig. For example, to update the SSH keys, create a file named update-ssh-keys.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: update-ssh-keys labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 passwd: users: - name: core sshAuthorizedKeys: - \"ssh-rsa \u003cyour-new-ssh-public-key\u003e\"\rReplace \u003cyour-new-ssh-public-key\u003e with the actual content of the new SSH public key.\nApply the MachineConfig: oc apply -f update-ssh-keys.yaml\rStep 3: Monitor the Status of the Machine Config Pool After applying the MachineConfig, you should monitor the status of the affected Machine Config Pool to ensure the changes are applied:\noc get machineconfigpool\rWait until the UPDATED status is True for the respective Machine Config Pool (e.g., worker or master).\nStep 4: Verify the Configuration Change SSH into one of the nodes to verify the configuration change: ssh core@\u003cnode-ip\u003e\rCheck the updated configuration as needed, for example, to verify SSH keys: cat ~/.ssh/authorized_keys\rStep 5: Restart Affected Services or Pods Some changes might require restarting affected services or pods. To restart the affected pods, you can use:\noc rollout restart deployment/\u003cdeployment-name\u003e -n \u003cnamespace\u003e\rImportant Notes Machine Config Operator: Be cautious when using the Machine Config Operator, as incorrect configurations can lead to node failures. Backup Existing Configurations: Always backup your existing configurations before making changes. Node Role: Make sure to specify the correct node role (master or worker) in the labels of your MachineConfig. Multiple Changes: If you have multiple changes to apply, consider creating a single MachineConfig that encapsulates all required changes, rather than creating multiple individual MachineConfigs. This guide provides a structured approach to changing configurations for all OpenShift nodes. If you have specific configurations in mind or need further assistance, feel free to ask!\nChanging Configuration for All OpenShift Nodes OpenShift 4 uses the Machine Config Operator (MCO) to manage the configuration of nodes. This guide provides steps to modify systemd services, adjust kernel parameters, and change default container runtime settings.\nModifying Systemd Services Step 1: Create a MachineConfig for Systemd Modifications Create a YAML file for the MachineConfig. For example, to modify a systemd service, create a file named modify-systemd.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: modify-systemd labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 systemd: units: - name: my-service.service enabled: true dropins: - name: 10-my-custom.conf contents: | [Service] Environment=\"MY_ENV_VAR=my_value\" Replace my-service.service and the Environment variables with your actual service name and configuration.\nStep 2: Apply the MachineConfig oc apply -f modify-systemd.yaml\rStep 3: Verify the Change SSH into the node and check the status of the modified service:\nssh core@\u003cnode-ip\u003e systemctl status my-service.service\rAdjusting Kernel Parameters Step 1: Create a MachineConfig for Kernel Parameters Create a YAML file for the MachineConfig. For example, to adjust kernel parameters, create a file named adjust-kernel-params.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: adjust-kernel-params labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 kernelArguments: - \"parameter1=value1\" - \"parameter2=value2\"\rReplace parameter1=value1 and parameter2=value2 with the actual kernel parameters you need to set.\nStep 2: Apply the MachineConfig oc apply -f adjust-kernel-params.yaml\rStep 3: Verify the Change SSH into the node and check the current kernel parameters:\nssh core@\u003cnode-ip\u003e sysctl -a | grep parameter1\rChanging the Default Container Runtime Settings Step 1: Create a MachineConfig for Container Runtime Settings Create a YAML file for the MachineConfig. For example, to change container runtime settings, create a file named change-runtime-settings.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: change-runtime-settings labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 storage: files: - path: /etc/crio/crio.conf mode: 0644 owner: root:root contents: inline: | [crio] default_runtime = \"my-runtime\" [runtime] runtime_type = \"my-runtime\" Replace my-runtime with your actual runtime settings.\nStep 2: Apply the MachineConfig oc apply -f change-runtime-settings.yaml\rStep 3: Verify the Change SSH into the node and check the runtime configuration:\nssh core@\u003cnode-ip\u003e cat /etc/crio/crio.conf\rStep 4: Monitor the Machine Config Pool After applying any of the MachineConfigs, monitor the Machine Config Pool:\noc get machineconfigpool\rWait until the UPDATED status is True for the respective Machine Config Pool (e.g., worker or master).\nImportant Notes:\nMachine Config Operator: Be cautious when using the MCO, as incorrect configurations can lead to node failures. Backup Existing Configurations: Always backup your existing configurations before making changes. Node Role: Ensure to specify the correct node role (master or worker) in the labels of your MachineConfig. Service Impact: Some changes may require restarting the affected services or pods for the changes to take effect. This guide provides clear instructions on modifying systemd services, adjusting kernel parameters, and changing container runtime settings in OpenShift nodes. If you have specific requirements or additional questions, feel free to ask!\nUpgrading OpenShift Cluster Upgrading an OpenShift cluster requires careful planning and execution. This guide outlines the steps for upgrading an OpenShift cluster.\nPrerequisites Backup Important Data:\nBackup your current cluster state, including etcd data and any custom resources. Review Release Notes:\nCheck the OpenShift Release Notes for any specific upgrade instructions and deprecated features. Check Compatibility:\nVerify that your current version is compatible with the target version. Update CLI Tools:\nEnsure you have the latest version of oc and openshift-install CLI tools that match the version you’re upgrading to. Step 1: Prepare for the Upgrade Check Cluster Health:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rDrain Nodes:\nStart with the master nodes, draining them to prepare for the upgrade. oc adm drain \u003cmaster-node-name\u003e --ignore-daemonsets --force --delete-local-data\rUpdate Machine Configurations (if needed):\nIf there are any machine configurations that need updates before the upgrade, apply those changes now. Step 2: Upgrade the Cluster Start the Upgrade Process:\nTrigger the upgrade process using the OpenShift CLI. oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e\rMonitor the Upgrade:\nCheck the status of the upgrade process. oc get clusterversion\rAllow Upgrade to Complete:\nWait for the upgrade process to complete. Ensure that all cluster operators are reporting as Available. Step 3: Post-Upgrade Tasks Uncordon Nodes:\nOnce the upgrade is complete, uncordon the master nodes and worker nodes. oc adm uncordon \u003cmaster-node-name\u003e oc adm uncordon \u003cworker-node-name\u003e\rCheck Cluster Health Again:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rUpdate Machine Configurations (if necessary):\nIf there are additional machine configurations or settings required for the new version, apply those now. Validate Application Functionality:\nVerify that all applications are functioning as expected after the upgrade. Step 4: Clean Up Remove Old Images:\nClean up unused images from the nodes to free up space. oc adm prune images\rMonitor Cluster Performance:\nKeep an eye on the cluster performance after the upgrade to ensure everything is running smoothly. Important Notes Upgrade Path: Always follow the recommended upgrade path (e.g., from 4.x to 4.y) as specified in the OpenShift documentation. Test Upgrades: If possible, test the upgrade process in a staging environment before performing it in production. Documentation: Refer to the official OpenShift Upgrade Documentation for detailed instructions specific to your version. This guide provides a structured approach to upgrading your OpenShift cluster. If you have specific requirements or encounter issues during the upgrade process, feel free to ask for assistance!\nUpgrading OpenShift Nodes in Batches When upgrading an OpenShift cluster with multiple nodes, you can upgrade nodes in batches (e.g., three at a time) to minimize downtime and ensure a smooth transition.\nPrerequisites Backup Your Cluster: Before proceeding, back up your etcd and any critical application data. Review Release Notes: Check the OpenShift release notes for any breaking changes or important notes about the upgrade. Upgrade Plan: Decide on the upgrade order for your nodes (typically, master nodes are upgraded first, followed by worker nodes). Step 1: Prepare for the Upgrade Check Cluster Health:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rDrain the Nodes:\nStart with the nodes you plan to upgrade. Drain three nodes at a time. for node in node1 node2 node3; do oc adm drain $node --ignore-daemonsets --force --delete-local-data done\rReplace node1, node2, and node3 with the actual node names.\nStep 2: Upgrade the Nodes Start the Upgrade Process:\nBegin the upgrade for the selected nodes. for node in node1 node2 node3; do oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e --node=$node done\rReplace \u003cnew-openshift-image\u003e with the image for the target OpenShift version.\nMonitor the Upgrade:\nCheck the status of the upgrade process for each node. oc get clusterversion\rWait until the upgrade process completes for all three nodes.\nStep 3: Uncordon the Nodes Uncordon the Upgraded Nodes: Once the upgrade is complete, uncordon the nodes to make them schedulable again. for node in node1 node2 node3; do oc adm uncordon $node done\rStep 4: Repeat for Remaining Nodes Continue Upgrading: Repeat the above steps for the next batch of three nodes until all nodes are upgraded. # Upgrade next batch for node in node4 node5 node6; do oc adm drain $node --ignore-daemonsets --force --delete-local-data oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e --node=$node oc adm uncordon $node done\rStep 5: Post-Upgrade Verification Check Cluster Health Again:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rValidate Application Functionality:\nEnsure that all applications are functioning as expected after the upgrade. Important Notes Plan for Downtime: Upgrading nodes in batches may cause temporary unavailability for applications running on those nodes. Ensure your applications are resilient and can handle node failures. Monitor Performance: After upgrading, monitor cluster performance and health metrics to ensure everything is running smoothly. Follow Upgrade Path: Always adhere to the recommended upgrade paths specified in the OpenShift documentation. By following this guide, you can effectively upgrade three nodes at a time in your OpenShift cluster. If you have any specific requirements or encounter issues during the upgrade process, feel free to ask for assistance!\nHow to change the no of node unavailable while upgrading oc patch mcp worker --type merge --patch '{\"spec\": {\"maxUnavailable\": 2}}'\rVerify the maxUnavailable value: oc get mcp worker -o yaml | grep maxUnavailable\rModifying OpenShift Project Templates OpenShift provides the ability to create and modify project templates, which define a standard set of resources and configurations that can be instantiated to create new projects or applications quickly. Modifying a project template allows you to customize default behaviors and settings for your applications.\nKey Components of a Project Template Parameters: Variables that can be customized when instantiating the template. Objects: The resources that will be created when the template is applied (e.g., DeploymentConfigs, Services, Routes). Labels and Annotations: Metadata for categorizing and describing resources. Viewing Existing Templates To view the templates available in your OpenShift environment, use:\noc get templates\rModifying a Template You can modify an existing template by following these steps:\n1. Export the Template To export a template to a file for editing:\noc get template \u003ctemplate-name\u003e -o yaml \u003e template.yaml\rReplace \u003ctemplate-name\u003e with the name of the template you want to modify.\n2. Edit the Template Open the template.yaml file in your preferred text editor and make the necessary changes. Key areas to consider modifying:\nParameters: Add, remove, or change parameter definitions. Objects: Modify or add resource specifications (e.g., change replicas in a DeploymentConfig). Labels/Annotations: Update or add labels and annotations for better resource management. 3. Apply the Modified Template Once you’ve made your modifications, you can apply the updated template back to OpenShift:\noc apply -f template.yaml\rAdding Resource Quotas Resource quotas are used to limit the amount of resources that can be consumed in a project. To add a resource quota to your template, include a ResourceQuota object in the objects section of the template:\napiVersion: v1 kind: ResourceQuota metadata: name: example-quota spec: hard: requests.cpu: \"4\" requests.memory: \"8Gi\" limits.cpu: \"4\" limits.memory: \"8Gi\"\rAdding LimitRanges Limit ranges set default request and limit values for containers in a project. You can add a LimitRange object in the objects section of the template:\napiVersion: v1 kind: LimitRange metadata: name: example-limits spec: limits: - default: cpu: \"500m\" memory: \"1Gi\" defaultRequest: cpu: \"250m\" memory: \"512Mi\" type: Container\rCreating a New Template If you want to create a new project template from scratch, you can use:\noc create -f \u003ctemplate-file\u003e.yaml\rMake sure to define parameters, objects, and metadata in the YAML file.\nExample Template Structure Here’s a simple example structure of a project template in YAML, including resource quotas and limit ranges:\napiVersion: template.openshift.io/v1 kind: Template metadata: name: example-template labels: app: example parameters: - name: APP_NAME description: The name of the application required: true objects: - apiVersion: v1 kind: ResourceQuota metadata: name: example-quota spec: hard: requests.cpu: \"4\" requests.memory: \"8Gi\" limits.cpu: \"4\" limits.memory: \"8Gi\" - apiVersion: v1 kind: LimitRange metadata: name: example-limits spec: limits: - default: cpu: \"500m\" memory: \"1Gi\" defaultRequest: cpu: \"250m\" memory: \"512Mi\" type: Container - apiVersion: apps/v1 kind: Deployment metadata: name: ${APP_NAME} spec: replicas: 1 selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} spec: containers: - name: ${APP_NAME} image: nginx ports: - containerPort: 80\rConclusion Modifying OpenShift project templates is a powerful way to standardize application deployments and ensure consistency across projects. By using parameters, object definitions, resource quotas, and limit ranges effectively, you can create flexible and reusable templates tailored to your organization’s needs.\nConfiguring a Node Selector for a Project oc adm new-project demo --node-selector \"tier=1\" oc annotate namespace demo openshift.io/node-selector=\"tier=2\" --overwrite\rApplying Quotas to Multiple Projects The following is an example of creating a cluster resource quota for all projects owned by the qa user: oc create clusterquota user-qa \\ \u003e --project-annotation-selector openshift.io/requester=qa \\ \u003e --hard pods=12,secrets=20\rThe following is an example of creating a cluster resource quota for all projects that have been assigned the environment=qa label: oc create clusterquota env-qa \\ \u003e --project-label-selector environment=qa \\ \u003e --hard pods=10,services=5\rExposing the Internal OpenShift Registry To expose the internal OpenShift registry externally:\nVerify Registry Status oc get pods -n openshift-image-registry\rExpose the Registry Service oc expose service image-registry -n openshift-image-registry --name=external-registry-route\rRetrieve the Route URL oc get route external-registry-route -n openshift-image-registry # Output should be a URL like: https://external-registry-route.\u003ccluster-domain\u003e\rLog in to the Registry using oc CLI oc login -u \u003cusername\u003e -p \u003cpassword\u003e oc registry login\rAlternatively, log in using Docker CLI oc whoami -t # Retrieve token docker login -u \u003cusername\u003e -p \u003ctoken\u003e https://external-registry-route.\u003ccluster-domain\u003e\rPush and Pull Images Push docker tag \u003cimage\u003e external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e docker push external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e\rPull docker pull external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e\rOverview of the openshift-config Namespace in OpenShift The openshift-config namespace in OpenShift is a critical system namespace that stores configuration resources essential for the cluster’s operation, including settings for authentication, networking, and security.\nKey Resources in openshift-config 1. Authentication and OAuth Configuration OAuth (OAuth CR): Manages user authentication for the cluster. Identity Providers: Configuration for providers like LDAP, GitHub, Google, OpenID Connect, etc. OAuth Tokens: Settings for tokens used by applications or users to authenticate with the API. 2. Ingress and Proxy Configuration Cluster Proxy (Proxy CR): Cluster-wide HTTP/HTTPS proxy settings for clusters behind firewalls or with limited internet access. Cluster Ingress (Ingress CR): Default settings for external application exposure and the default domain for routes. 3. Network Configuration DNS ConfigMap: Custom DNS configuration required by cluster applications. Cluster-wide Network Settings: Configures network policies, network isolation, and other networking settings. 4. Certificate and Secret Management Certificates for Internal Services: TLS certificates for internal cluster services like the API server and internal registry. Trusted CA Bundle: CA bundle configuration for trusting external/internal certificates. 5. Image Registry Configuration Registry ConfigMap: Settings for the internal OpenShift image registry (e.g., storage backend and access control). Image Content Source Policies: Policies controlling allowed registries for image pulling and storage. 6. Etcd Configuration Etcd Settings: Custom settings for the etcd data store, critical for OpenShift’s cluster state and configuration. 7. Cluster Operators Configuration Configuration for various cluster operators affecting core components like networking, ingress, and monitoring. 8. Custom Configurations for OpenShift Components Holds configurations for specific deployment needs, security hardening, or custom operational policies. Summary of Key Resource Types in openshift-config ConfigMaps Secrets Custom Resources: Like OAuth, Ingress, and Proxy These configurations in openshift-config are essential for managing the functionality, security, and accessibility of the OpenShift cluster.",
    "description": "Openshift",
    "tags": [],
    "title": "Openshift 4 Tasks",
    "uri": "/ocp/ocp-tasks/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "OpenShift Deployment Scenarios OpenShift on AWS Cloud\nDeploying Red Hat OpenShift on Amazon Web Services (AWS) to leverage scalability and cloud-native integrations.\nOpenShift on Azure Cloud\nRunning OpenShift on Microsoft Azure for seamless integration with Azure services and enterprise-grade security.\nOpenShift with CodeReady Containers (CRC)\nA lightweight, local development environment using CRC, suitable for testing and learning OpenShift features on a single machine.\nOpenShift Single Node (SNO) Deployment\nInstalling OpenShift in a single-node configuration, ideal for edge and resource-constrained environments.\nOpenShift on Bare Metal\nBare metal deployment for high performance and direct hardware access, commonly used in on-premise data centers.\nOpenShift on VMware Infrastructure\nInstalling OpenShift in virtualized environments using VMware vSphere for enterprise-grade resource management.\nOpenShift Air-Gapped Installation\nDeploying OpenShift in disconnected or highly secure environments without direct internet access.",
    "description": "OpenShift Deployment Scenarios OpenShift on AWS Cloud\nDeploying Red Hat OpenShift on Amazon Web Services (AWS) to leverage scalability and cloud-native integrations.\nOpenShift on Azure Cloud\nRunning OpenShift on Microsoft Azure for seamless integration with Azure services and enterprise-grade security.\nOpenShift with CodeReady Containers (CRC)\nA lightweight, local development environment using CRC, suitable for testing and learning OpenShift features on a single machine.\nOpenShift Single Node (SNO) Deployment\nInstalling OpenShift in a single-node configuration, ideal for edge and resource-constrained environments.",
    "tags": [],
    "title": "OpenShift Deployment",
    "uri": "/openshift-installation/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e OpenShift Deployment",
    "content": "Introduction CodeReady Containers (CRC) provides a minimal OpenShift 4 cluster for development and testing purposes on a local machine. It is ideal for learning, experimenting with OpenShift features, and developing operators or containerized applications.\nKey Features Single-node OpenShift 4 cluster Suitable for laptops and desktops Includes OpenShift CLI (oc) and Kubernetes CLI (kubectl) Easy setup with pre-built VM images System Requirements Component Requirement OS Linux, macOS, or Windows 10/11 RAM 9 GB minimum (12+ GB recommended) CPU 4 cores minimum Disk Space 35 GB free disk space Virtualization KVM (Linux), Hyper-V (Windows), HyperKit (macOS) Installation Steps 1. Download CRC Go to the official Red Hat CRC page:\n👉 https://developers.redhat.com/products/codeready-containers\nChoose the appropriate version for your OS.\n2. Extract and Install tar -xvf crc-linux-amd64.tar.xz sudo mv crc-linux-*-amd64/crc /usr/local/bin/\r3 Setup CRC This prepares the host system (adds DNS settings, creates required users/groups, etc.).\nSetup CRC\r4. Start CRC crc start\r5. Access OpenShift OpenShift Console: https://console-openshift-console.apps-crc.testing Default credentials: Username: developer Password: developer",
    "description": "Introduction CodeReady Containers (CRC) provides a minimal OpenShift 4 cluster for development and testing purposes on a local machine. It is ideal for learning, experimenting with OpenShift features, and developing operators or containerized applications.\nKey Features Single-node OpenShift 4 cluster Suitable for laptops and desktops Includes OpenShift CLI (oc) and Kubernetes CLI (kubectl) Easy setup with pre-built VM images System Requirements Component Requirement OS Linux, macOS, or Windows 10/11 RAM 9 GB minimum (12+ GB recommended) CPU 4 cores minimum Disk Space 35 GB free disk space Virtualization KVM (Linux), Hyper-V (Windows), HyperKit (macOS) Installation Steps 1. Download CRC Go to the official Red Hat CRC page:\n👉 https://developers.redhat.com/products/codeready-containers",
    "tags": [],
    "title": "OpenShift with CodeReady Containers (CRC)",
    "uri": "/openshift-installation/crc/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation \u003e OpenShift Deployment",
    "content": "Prerequisites Follow the GitHub Repository\nocp4-metal-install Create a RHEL 8 or 9 Virtual Machine on VMware Workstation\nEnsure the VM has two network interfaces (NICs) configured as Bridge: NIC 1: Connected to the public network (Internet access) NIC 2: Used for internal network communication Assign a static IP (e.g., 192.168.22.1/24) Learn how to generate and manage MAC addresses Understand Firewalld Configuration\nLearn about: public and private zones How to open ports and allow services Install and Configure a DHCP Server\nLearn how to assign static IPs using MAC addresses Test by confirming DHCP leases to other VMs Set Up DNS (BIND/named) on RHEL 8\nMake necessary DNS configuration changes Verify DNS resolution from other VMs Learn HAProxy\nUnderstand its role in API load balancing and routing Set Up an HTTP Server with a Custom Port\nHost sample content Access the hosted content from other VMs Configure NFS for Storage Class Usage\nSet System Timezone to UTC\ntimedatectl sudo timedatectl set-timezone UTC\rCheck Network Devices\nnmcli device status nmcli connection show\rActivate a Network Connection\nnmcli connection up \u003cconnection-name\u003e\rAdd a New Network Profile (If IP is not assigned)\nnmcli con add type ethernet ifname ens160 con-name ens160\rBring the Connection Up\nnmcli con up ens160\rConfigure Local YUM Repository Using ISO Mount the ISO Image\nmount /dev/sr0 /mnt/\rCreate a Local YUM Repository File\ncat \u003c\u003c EOF \u003e /etc/yum.repos.d/rhel8-local.repo [BaseOS] name=RHEL8 BaseOS baseurl=file:///mnt/BaseOS enabled=1 gpgcheck=0 [AppStream] name=RHEL8 AppStream baseurl=file:///mnt/AppStream enabled=1 gpgcheck=0 EOF",
    "description": "Prerequisites Follow the GitHub Repository\nocp4-metal-install Create a RHEL 8 or 9 Virtual Machine on VMware Workstation\nEnsure the VM has two network interfaces (NICs) configured as Bridge: NIC 1: Connected to the public network (Internet access) NIC 2: Used for internal network communication Assign a static IP (e.g., 192.168.22.1/24) Learn how to generate and manage MAC addresses Understand Firewalld Configuration\nLearn about: public and private zones How to open ports and allow services Install and Configure a DHCP Server",
    "tags": [],
    "title": "OpenShift on Baremetal",
    "uri": "/openshift-installation/bm/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
