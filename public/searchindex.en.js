var relearn_searchindex = [
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "OpenShift Prerequisites Before installing OpenShift, ensure the following prerequisites are met for your environment.\n1. Red Hat Account A Red Hat account is required to download the installer, access the pull secret, and manage subscriptions.\nüëâ Create a Red Hat Account\n2. OpenShift Installer Download the OpenShift installer appropriate for your target environment (bare metal, VMware, AWS, Azure, or GCP).\nüëâ Download the Installer\n3. Pull Secret The pull secret is required to access Red Hat‚Äôs container registries during installation.\nüëâ Download Pull Secret\n4. DNS and Networking Ensure DNS resolution is properly configured for the cluster. Required DNS records: api.\u003ccluster_name\u003e.\u003cbase_domain\u003e ‚Üí Load balancer or bootstrap node *.apps.\u003ccluster_name\u003e.\u003cbase_domain\u003e ‚Üí Application load balancer Configure reverse DNS lookup for all nodes. Ensure required firewall ports are open (API, ingress, node-to-node communication).\nüëâ OpenShift Networking Requirements 5. Infrastructure (VMs or Bare Metal) Provision infrastructure based on your target platform:\nBare Metal Minimum of 3 master nodes (control plane) Worker nodes (based on workload requirements) Hardware (per node): Master: 4 vCPU, 16 GB RAM, 120 GB disk Worker: 4 vCPU, 8 GB RAM, 120 GB disk VMware vSphere vCenter access with admin privileges Pre-created VM templates or ability to create VMs Shared datastore and networking Public Clouds (AWS, Azure, GCP) Proper IAM roles and permissions Region and VPC/subnet prepared Load balancers enabled Storage services (EBS, Azure Disk, GCP Persistent Disk) available 6. Cloud CLI Tools AWS CLI üëâ Download AWS CLI\nAzure CLI üëâ Download Azure CLI\nGCP SDK üëâ Download Google Cloud SDK\nVMware CLI Tools ovftool for importing templates vSphere client access 7. Certificates and Security Configure TLS certificates (self-signed or custom CA if required). Ensure SSH key pair is generated for cluster node access.\nüëâ Generate SSH Key 8. Storage Requirements Infrastructure storage for etcd and control plane. Persistent storage for applications (NFS, Ceph, GlusterFS, EBS, Azure Disk, GCP PD, VMware vSAN).\nüëâ Storage Requirements 9. Operating System Requirements Supported OS for nodes: Red Hat Enterprise Linux CoreOS (RHCOS) ‚Äì default Red Hat Enterprise Linux (RHEL) with prerequisites installed Ensure subscription is attached for RHEL nodes. 10. Additional Tools (Optional but Recommended) oc CLI (OpenShift client) üëâ Download OC CLI kubectl CLI (for Kubernetes-level operations) Infrastructure automation tools (Terraform, Ansible) for IaC deployments ‚úÖ With these prerequisites completed, you are ready to begin the OpenShift installation on your target platform.",
    "description": "Essential prerequisites for installing OpenShift across different platforms (bare metal, VMware, AWS, Azure, GCP).",
    "tags": [],
    "title": "OpenShift Prerequisites",
    "uri": "/ocp/prerequisites/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Redhat Openshift ",
    "uri": "/ocp/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "OpenShift Single Node Installation Guide (Agent-Based Assisted Installation) Prerequisites Virtualization Software (choose one): VMware Workstation VirtualBox Hyper-V VMware vSphere Virtual Machine Requirements: Minimum 8 vCPUs, 32 GB RAM, and 120 GB storage (SSD recommended). Developer Account on Red Hat: cloud.redhat.com Installation Steps Log in to cloud.redhat.com and navigate to Services.\nSelect OpenShift and click Create Cluster.\nFill in the required details.\nProvide your SSH public key and download the generated ISO image.\nCreate a new VM using the downloaded ISO image.\nAssign 8 vCPUs, 32 GB RAM, and 120 GB storage. Once the cluster is deployed, the details will be visible in the Red Hat Console.\nDownload the kubeconfig file and kubeadmin credentials.\nPost-Installation Tasks (You can include cluster configuration, storage setup, monitoring, and user management here.)\nOpenShift and Azure AD Integration Prerequisites Azure AD Tenant with required user groups. A running OpenShift cluster. Integration Steps 1. Register an Application in Azure AD Log in to the Azure Portal. Navigate to Azure Active Directory ‚Üí App registrations ‚Üí New registration. Enter a name (e.g., OpenShiftApp). Set the Redirect URI to the OpenShift callback URL:\nhttps://\u003copenshift-cluster\u003e/oauth2callback Click Register. 2. Configure Azure AD for OpenShift OAuth 2.1 Create a Client Secret\nIn the registered app, go to Certificates \u0026 Secrets ‚Üí New client secret. Add a description, choose an expiration period, and click Add. Copy and securely store the secret value. 2.2 Assign API Permissions\nNavigate to API permissions ‚Üí Add a permission. Select Microsoft Graph ‚Üí Delegated permissions. Add User.Read and any other required permissions. 3. Configure OpenShift OAuth for Azure AD On the OpenShift master node, edit the OAuth configuration:\napiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster annotations: release.openshift.io/create-only: \"true\" spec: identityProviders: - name: azuread mappingMethod: claim type: OpenID openID: clientID: \u003capplication-id\u003e clientSecret: name: azuread issuer: https://login.microsoftonline.com/\u003ctenant-id\u003e/v2.0 claims: email: - email id: - sub name: - name preferredUsername: - email",
    "description": "Step-by-step guide for deploying and integrating OpenShift Single Node clusters.",
    "tags": [],
    "title": "OpenShift Single Node Cluster",
    "uri": "/ocp/ocp-sno/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "Description This document describes the installation of OCP4 on the VMWare infra hosted on premise.\nRBAC required to manage the installation Be sure to be able to access to :\nthe dedicated vCenter https://vcenter01.local.com the bastion helper.local.com Informations about DNS The openshift will be reachable from Internet and from the LAN. It will use a public DNS record.\nA subzone of local.com. named ocp.local.com delgated to our team and is managed in an Azure DNS zone to give some freedom and responsivity when we need to manage it.\nIssues known During the installation of the production env, we were hit by the following bug : https://bugzilla.redhat.com/show_bug.cgi?id=1882022 .\nTo figure out we applied the following solution: https://access.redhat.com/solutions/5507161\nInformations required by the installer for the PRODUCTION key value basedomain local.com metadata.name ocp4 compute.hyperthreading Enabled compute.name worker compute.replicas 3 compute.platform.vsphere.cpus 8 compute.platform.vsphere.coresPerSocket 1 compute.platform.vsphere.memoryMB 16384 compute.platform.vsphere.osDisk.diskSizeGB 120 controlPlane.hyperthreading Enabled controlPlane.name master controlPlane.replicas 3 controlPlane.platform.vsphere.cpus 8 controlPlane.platform.vsphere.coresPerSocket 1 controlPlane.platform.vsphere.memoryMB 16384 controlPlane.platform.vsphere.osDisk.diskSizeGB 120 platform.vsphere.vcenter vcenter01.local.com platform.vsphere.username local.com\\administrator platform.vsphere.password find it in the secure place platform.vsphere.datacenter platform.vsphere.folder platform.vsphere.defaultDatastore ocp platform.vsphere.network ocp platform.vsphere.cluster ocp platform.vsphere.api_vip 192.168.1.100 platform.vsphere.ingress_vip 192.168.1.101 fips false pullSecret find it by going on https://cloud.redhat.com/openshift/create, navigate to the ‚ÄúDataCenter‚Äù tab, select ‚Äúvsphere‚Äù, and ‚Äúcopy your pull secret sshKey '' networking.clusterNetwork.cidr 172.20.0.0/14 networking.clusterNetwork.hostPrefix 23 networking.clusterNetwork.cidr 172.20.0.0/14 networking.clusterNetwork.hostPrefix 23 networking.serviceNetwork 172.19.0.0/16 Example of install-config.yaml apiVersion: v1 baseDomain: local.com compute: - hyperthreading: Enabled name: worker replicas: 3 platform: vsphere: cpus: 8 coresPerSocket: 1 memoryMB: 16384 osDisk: diskSizeGB: 120 controlPlane: hyperthreading: Enabled name: master replicas: 3 platform: vsphere: cpus: 8 coresPerSocket: 1 memoryMB: 16384 osDisk: diskSizeGB: 120 metadata: name: ocp4 platform: vsphere: vcenter: vcenter01.local.com username: password: XXXXXXXXXXXXXXXXXXXXXXX datacenter: folder: defaultDatastore: network: cluster: apiVIP: ingressVIP: fips: false pullSecret: '{\"auths\":XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX}}}' sshKey: networking: clusterNetwork: - cidr: 172.20.0.0/14 hostPrefix: 23 serviceNetwork: - 172.19.0.0/16\rThe VCenter certificate must be trusted (once !): curl -kSL https://vcenter01.local.com/certs/download.zip -o download.zip unzip download.zip sudo cp certs/lin/* /etc/pki/ca-trust/source/anchors sudo update-ca-trust extract\rPrerequisites have trusted the certificates on the bastion create an install-config.yaml file respecting the values described above In the VCenter, manually create the Folder that will be used by the installer.\nFor example, for creating the production env folder, go on the VMs and templates Reserve 2 static IPs that will be used by the ingress and the API create the matching Resource Records in the Azure DNS zone Create advanced network configuration Connect to the bastion ssh root@\u003cbastion\u003e\rCreate a working directory mkdir -p vsphere/$(date -I) cd vsphere\rcopy the install-config.yaml file you created in the working directory cp install-config.yaml $(date -I)/ tolerations: - effect: NoSchedule key: node-role.kubernetes.io/infra operator: Exists\rdownload the installer program. You can find all archived 4 versions under the following URL: https://mirror.openshift.com/pub/openshift-v4/clients/ocp\nWhile writting this doc, the version we deployed in other production env is the 4.6.18. vsphere $ wget https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.6.18/openshift-install-linux-4.6.18.tar.gz vsphere $ tar xzvf openshift-install-linux-4.6.18.tar.gz\rcreate the manifests file: ./openshift-install create manifests --dir=$(date -I)/\rCreate a file that is named cluster-network-03-config.yml touch $(date -I)/manifests/cluster-network-03-config.yml Add the following content in this file apiVersion: operator.openshift.io/v1 kind: Network metadata: name: cluster spec: clusterNetwork: - cidr: 172.20.0.0/14 hostPrefix: 23 serviceNetwork: - 172.19.0.0/16 defaultNetwork: type: OVNKubernetes ovnKubernetesConfig: mtu: 1400 genevePort: 6081 Run the installer from the same working directory you created during the advanced network config step, run the openshift install tool ./openshift-install create cluster --dir=./$(date -I) --log-level=info\rIn production env it took around 40 minutes to be run:\nINFO Creating infrastructure resources... INFO Waiting up to 20m0s for the Kubernetes API at https://api.ocp4.local.com:6443... INFO API v1.19.0+f173eb4 up INFO Waiting up to 30m0s for bootstrapping to complete... INFO Destroying the bootstrap resources... INFO Waiting up to 40m0s for the cluster at https://api.ocp4.local.com:6443 to initialize... INFO Waiting up to 10m0s for the openshift-console route to be created... INFO Install complete! INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/ocpadmin/vsphere/2021-03-25/auth/kubeconfig' INFO Access the OpenShift web-console here: https://console-openshift-console.apps.ocp4.local.com INFO Login to the console with user: \"kubeadmin\", and password: \"XXXXXXXXXXXX\" INFO Time elapsed: 38m24s\rinstall the oc client on the bastion (once) download and install oc curl https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.6.18/openshift-client-linux-4.6.18.tar.gz -o openshift-client-linux-4.6.18.tar.gz tar xzvf openshift-client-linux-4.6.18.tar.gz sudo cp oc /usr/local/bin/ sudo cp kubectl /usr/local/bin/\rManage the auto completion sudo yum install -y bash-completion oc completion bash \u003e ~/.kube/completion.bash.inc printf \" # Kubectl shell completion source '$HOME/.kube/completion.bash.inc' \" \u003e\u003e $HOME/.bash_profile source $HOME/.bash_profile\rRegistry installation temporarily decrease the registry replicas to 1 oc patch config.imageregistry.operator.openshift.io/cluster --type=merge -p '{\"spec\":{\"rolloutStrategy\":\"Recreate\",\"replicas\":1}}'\rDefine a PVC of 100Gi in a ‚Äòpvc.yaml‚Äô file kind: PersistentVolumeClaim apiVersion: v1 metadata: name: image-registry-storage spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi create it in the right namespace oc create -f pvc.yaml -n openshift-image-registry edit the registry configuration oc edit config.imageregistry.operator.openshift.io -o yaml\rand modify the spec.storage configuration to match the following settings: storage: pvc: claim: image-registry-storage Wait for the PV has been successfully created and make the registry managed oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{\"spec\":{\"managementState\":\"Managed\"}}'\rWait for the registry becomes ready watch oc get po -l docker-registry=default Check the cluster operator became ready in a non-degraded state oc get co image-registry\rTest the registry External references",
    "description": "Comprehensive OpenShift guides and documentation.",
    "tags": [],
    "title": "Openshift Installation on Vmware",
    "uri": "/ocp/ocp-onvmware-copy/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "Red Hat Openshift 4 Bare Metal OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 12: Create other OpenShift Users Step 13: Configure storage for the Image Registry Step 14: Troubleshooting Installation Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "description": "Red Hat Openshift 4 Bare Metal OpenShift 4 Bare Metal Install User Provisioned Infrastructure (UPI) Step 0 : Architecture Diagram Step 1: Setup KVM Infrastructure (On Hypervisor Node) Step 2: Create utility Virtual Machine Step 4: Install and Configure DHCP server Step 5: Configure OCP Zone on Bind DNS Server Step 6: Setup TFTP Service Step 7: Configure HAProxy as Load balancer Step 8: Install OpenShift installer and CLI binary Step 9: Generate ignition files Step 10: Create Bootstrap, Masters and Worker VMs (On Hypervisor Node) Step 11: Login to OpenShift CLI / Web Console Step 12: Create other OpenShift Users Step 13: Configure storage for the Image Registry Step 14: Troubleshooting Installation Step 15: OpenShift Virtualization - Run VMs on OpenShift with KubeVirt - Windows Server Step 16: NFS Storage Class with OpenShift \u0026 Kubernetes",
    "tags": [],
    "title": "Openshift Baremetal",
    "uri": "/ocp/openshift-baremetal.md/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "Table of Contents Pre Assessment - 20 Questions Introduction to OpenShift Overview of OpenShift: A comprehensive platform for developing, deploying, and managing containerized applications. Benefits of Using OpenShift: Scalability, flexibility, and ease of integration with CI/CD pipelines. Setting Up Your Environment Prerequisites: System requirements and necessary software. Installing OpenShift CLI (oc): Step-by-step guide for installation. Configuring Local Development Environments CodeReady Containers (CRC) What is CRC?: A tool for running OpenShift locally. Setting Up CRC Locally: Installation and configuration. Managing CRC Environments: Commands and best practices. Accessing the OpenShift Web Console: Navigating the UI for managing applications. Understanding OpenShift Architecture Key Components: Overview of Pods, Services, and Routes. OpenShift vs. Kubernetes: Differences and use cases. Creating and Managing Applications Building Applications: Development workflows in OpenShift. Source-to-Image (S2I) Process: Automating image creation from source code. Deploying Applications on OpenShift: Methods for deploying applications. Working with Containers Understanding Containers and Images: Core concepts and terminology. Managing Image Streams: Working with image repositories. Using Dockerfiles and Build Configurations: Customizing builds and images. Application Scaling and Management Scaling Applications: Manual and automatic scaling techniques. Rolling Updates and Rollbacks: Strategies for updating applications safely. Health Checks and Readiness Probes: Ensuring application reliability. Autoscaling Applications: Implementing horizontal and vertical scaling. Security in OpenShift Role-Based Access Control (RBAC): Managing user permissions and roles. Securing Applications and Data: Best practices for data security. Network Security Practices: Protecting application communication. Security Contexts and Policies: Configuring security for pods. Service Account: Managing service accounts for security. Networking in OpenShift Services and Routes: Exposing applications to the network. Configuring Ingress: Managing external access to services. Network Policies: Controlling traffic between pods. Persistent Storage in OpenShift Working with Persistent Volumes (PVs) and Persistent Volume Claims (PVCs): Managing storage. Storage Classes: Defining different types of storage. Monitoring and Logging Accessing Application Logs: Viewing logs for debugging. Integrating with Monitoring Tools: Tools for performance monitoring. Setting Up Alerts: Configuring alerts for important events. Metrics Collection and Visualization: Tools and techniques for metrics. CI/CD Integration Setting Up Pipelines using Jenkins: Automating the build and deployment process. Best Practices for Development Application Design Patterns: Effective design strategies. Managing Configurations: Keeping configurations organized. Troubleshooting Applications Common Issues and Solutions: Identifying and resolving frequent problems. Debugging Techniques: Tools and methods for troubleshooting. Managing Configuration Data Secrets and ConfigMaps Creating and Using Secrets: Storing sensitive information securely. Creating and Using ConfigMaps: Managing non-sensitive configuration data. Best Practices for Managing Sensitive Data: Ensuring security. Operators in OpenShift What are Operators?: Automating application management. Managing Operators with OperatorHub: Installing and configuring operators. Scheduling Options Understanding Scheduling in OpenShift: Basics of pod scheduling. Affinity and Anti-Affinity Node Affinity: Specifying node preferences. Pod Affinity: Co-locating pods based on criteria. Pod Anti-Affinity: Avoiding pod co-location. Taints and Tolerations What are Taints and Tolerations?: Managing node availability. Configuring Taints and Tolerations: Best practices. Using NodeSelectors: Directing pods to specific nodes. Scheduling with CronJobs Creating CronJobs for Scheduled Tasks: Automating recurring tasks. Post Assesment: 30 Questions",
    "description": "OpenShift for Developers",
    "tags": [],
    "title": "OpenShift for Developers",
    "uri": "/ocp/openshift-developer/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "Extending default monitoring stack in openshift 4 https://access.redhat.com/solutions/5298651\rHow to list internal registry repositories and images in OpenShift 4 https://access.redhat.com/solutions/5177301\rHow to generate a sosreport within nodes without SSH in OCP 4 https://access.redhat.com/solutions/4387261\rMonitoring operator is degraded with message ‚Äúupdates to statefulset spec for fields are forbidden https://access.redhat.com/solutions/6529271\rPreparing to upgrade to OpenShift Container Platform 4.9 https://access.redhat.com/articles/6329921\rFailing to create pod sandbox on OpenShift 3 and 4 https://access.redhat.com/solutions/4321791\rConsolidated Troubleshooting Article OpenShift Container Platform 4.x https://access.redhat.com/articles/4217411\rHow to rename identity provider in openshift https://access.redhat.com/solutions/3300361\rHow to re-create kubeconfig for system:admin user in Openshift 4 https://access.redhat.com/solutions/5286371\rBackend Performance Requirements for OpenShift etcd https://access.redhat.com/solutions/4770281\rHow to graph etcd metrics using Prometheus to gauge Etcd performance in OpenShift https://access.redhat.com/solutions/5489721\rHow to delete all kubernetes.io/events in etcd https://access.redhat.com/solutions/6171352\rHow to fix error on PVC https://devopstales.github.io/kubernetes/openshift-rbd-fsck/\rHow to update ssh keys for Openshift master nodes. https://access.redhat.com/solutions/3868301\rconfigure Alertmanager https://access.redhat.com/solutions/4882271",
    "description": "OpenShift Tasks",
    "tags": [],
    "title": "OpenShift Tasks",
    "uri": "/ocp/openshifttasks/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "OpenShift-customized-Training Prerequisites Aws/Azure Account Resources used 3 LoadBalancers Route 53 AWS s3 bucket 1 VPC 5 Ip addresses NAT gateways 8 VMS with 4 cpu and 8 GB of RAM each Duration:\nOpenShift Container Platform architecture\nOverview of Red Hat Enterprise Linux CoreOS (RHCOS)\nCrio OverView\nPodman Overview\nOverview of journactl\nHands on Lab\nCrio Podman Journalctl Installation and update\nUnderstand the underline infrstracture/resources requirements. Know Quay.io Know Redhat Registry know ignition files Installation with IPI Installation with user-provisioned infrastructure Installation on vmware(optional) Multi tenant Openshift Installation Configure Network Policy Installation with customized network plugins Troubleshooting installation issues Gathering logs from a failed installation Manually gathering logs with SSH access to your host(s) Manually gathering logs without SSH access to your host(s) Getting debug information from the installation program Post_installation_configuration This task will take most of the time\nConfiguration of Authentication with Htpasswd\nConfiguration of Authentication with Azure AD\nRemove the default virtual admin user (kubeadmin)\nSecure Api with ssl certificate\nSecure Route with Route\nSetting the Ingress Controller\nRestricting the API server to private\nConfiguration Default Quota project template\nConfigure default limits\nRestrict user for LoadBalancer service.\nConfigure Alert Manager\nUpdating the global cluster pull secret\nConfigure Autoscaling for nodes\nCreate infrastracture nodes\nMove all infr related services to infra nodes\nOpenShift Internal Registry\nRouter pods\nMonitoring pods\nLogging pods\nOpenShift Backup and DR\nInstallation and Configuration of Kasten/Velero Setup the backup of etcd Recovering from the etcd backup Post-installation node tasks\nAdding RHEL compute nodes to a cluster if needed Configuring Machine health checks Limitations when deploying machine health checks Node host best practices Configure different type of profile Updating ssh keys for master and worker nodes Post-installation network configuration\nEnabling the cluster-wide proxy Configuring ingress cluster traffic Configuring network policy Configuring multitenant isolation by using network policy Post-installation storage configuration\nDynamic provisioning Defining a storage class Using Azure file for RWX Installation and configuration cephcluster with rook operator to achieve below: Block storage File storage Object Storage Know OpenShift Internal Registry\nConfiguring additional trust stores for image registry access Configuring storage credentials for the Image Registry Operator OpenShift Scc\nUnderstanding default scc Creating and user custom scc Pod Scheduling*\nDefault scheduling Infrastructure Topological Levels Affinity Anti Affinity Advanced scheduling Pod Affinity and Anti-affinity Node Affinity Node Selectors Taints and Tolerations Custom scheduling Deploying the Scheduler Troubleshoot\nPod related issues\nRouter/Registry Not deploying to correct node Registry not showing contents of NFS mount (persistent volume) Hosts Can No Longer Resolve Each Other During Anisble Install Failure to deploy registry (permissions issues) Application Pod fails to deploy Issues with Nodes\nNodes being reported as ready, but builds failing Node reporting NotReady Nodes report ready but ETCD health check fails Atomic-openshift-node service fails to start Registry issues\nOpenShift builds fail trying to push image using a wrong IP address for the registry OpenShift build error: failed to push image while using NFS persistent storage Failure to push image to OpenShift‚Äôs Registry when backed by shared storage Quotas and Limitranges\nMust make a non-zero request for cpu Installation Fails‚Ä¶‚Äã\nWeb Console Public URL on a different Port UI Redirecting to the URL of the masters instead of the LB Intermittent Login issues (htpasswd) Build Issues oc new-app runs s2i instead of Docker build Binary Build Fails, citing ‚ÄúBadRequest‚Äù Issues related to Identity\nuser is unable to login user has two identities How to impersonate user login with service account Migration from Ocp3 to ocp 4",
    "description": "Openshift TOC",
    "tags": [],
    "title": "Openshift TOC",
    "uri": "/ocp/toc/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "Openshift Authentication and Authorization Htpasswd Configuration: yum install httpd* htpasswd -c -B -b /tmp/htpasswd student redhat123 htpasswd -B -b /tmp/htpasswd student2 redhat123 oc create secret generic htpasswd-secret --from-file htpasswd=/tmp/htpasswd -n openshift-config vi oauth.yaml apiVersion: config.openshift.io/v1 kind: OAuth metadata: name: cluster spec: identityProviders: - name: ldap mappingMethod: claim type: HTPasswd htpasswd: fileData: name: htpasswd-secret\rConplete this as well https://gitlab.com/gcpnirpendra/openshift-280-training/-/issues/7\rProject How to create a project oc new-project myapp\rCheck all the project you have got access to oc get projects\rCheck your current project oc project\rGive access to Project from UI How to create a pod oc run \u003cname\u003e --image=nginx\rAnother example vi pod.yanl\rapiVersion: v1\rkind: Pod\rmetadata:\rname: firstpod\rspec:\rcontainers:\r- name: firstcontainer\rimage: nginx\rHow to check all pods oc get pods\rHow to check all pods in all namespaces oc get pods -A\rHow to check the name of all pods oc get pods -o name\rHow to check the labels of all pods running oc get pods --show-lables\rHow to login to a pods oc exec -it podname sh How to check the logs of a pods oc logs podname\rHow to login to a pod oc exec -it podname sh\rHow to delete a pod oc delete pod \u003cpodname\u003e\rHow to delete a pod forcefully oc delete pod --force --grace-period=0\rHow to delete multiple pod in one go for i in $(oc get pods -o name ); do oc delete pod $i ;done How to check the logs for a specific container oc logs podname -c \u003ccontainer-name\u003e\rHow to login to specific container oc exec -it podname -c \u003ccontainer-name\u003e\rService How to check all services oc get svc\rHow to create a service oc expose pod/deployment deployment/myapp --port=80\rService with yaml manifest apiVersion: v1\rkind: Service\rmetadata:\rname: my-service\rspec:\rselector:\rapp: MyApp\rports:\r- protocol: TCP\rport: 80\rtargetPort: 9376\rHow to check the labels where the service is forwarding requests oc describe svc \u003csvcname\u003e\rHow to check the endpoints of a service oc get ep\rDeployments How to create a deployment oc create deployment myapp --image=nginx\rRolling update ‚Äúwww‚Äù containers of ‚Äúfrontend‚Äù deployment, updating the image oc set image deployment/frontend www=image:v2 Check the history of deployments including the revision oc rollout history deployment/frontend Rollback to the previous deployment oc rollout undo deployment/frontend Rollback to a specific revision oc rollout undo deployment/frontend --to-revision=2 Watch rolling update status of ‚Äúfrontend‚Äù deployment until completion oc rollout status -w deployment/frontend Rolling restart of the ‚Äúfrontend‚Äù deployment oc rollout restart deployment/frontend Secrets Check all secrets oc get secrets\rHow to create secret oc create secret tls my-tls-secret \\\r--cert=path/to/cert/file \\\r--key=path/to/key/file\rCreate Secret for username and password oc create secret generic creds --from-literal=name=nippy --from=literal=pass=123\rList the environment variables defined on all pods oc set env pods --all --list\rImport environment from a secret oc set env --from=secret/mysecret dc/myapp\rImport environment from a config map with a prefix oc set env --from=configmap/myconfigmap --prefix=MYSQL_ dc/myapp\rRemove the environment variable ENV from container ‚Äòc1‚Äô in all deployment configs oc set env deployments --all --containers=\"c1\" ENV-\rConfigMaps List all configmaps oc get cm\rCreate a new configmap oc create configmap game-config --from-file=configure-pod-container\rGet the yaml file for the configmap oc get configmaps game-config -o yaml\rUse configmap as a volume in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: dapi-test-pod\rspec:\rcontainers:\r- name: test-container\rimage: k8s.gcr.io/busybox\rcommand: [ \"/bin/sh\", \"-c\", \"ls /etc/config/\" ]\rvolumeMounts:\r- name: config-volume\rmountPath: /etc/config\rvolumes:\r- name: config-volume\rconfigMap:\r# Provide the name of the ConfigMap containing the files you want\r# to add to the container\rname: special-config\rrestartPolicy: Never\rVolumes emptyDir Example apiVersion: v1\rkind: Pod\rmetadata:\rname: test-pd\rspec:\rcontainers:\r- image: registry.k8s.io/test-webserver\rname: test-container\rvolumeMounts:\r- mountPath: /cache\rname: cache-volume\rvolumes:\r- name: cache-volume\remptyDir:\rsizeLimit: 500Mi\rCheck all pvcs oc get pvc\rCreate a pvc apiVersion: v1\rkind: PersistentVolume\rmetadata:\rname: pv0001 spec:\rcapacity:\rstorage: 5Gi accessModes:\r- ReadWriteOnce use pvc inside a pod kind: Pod\rapiVersion: v1\rmetadata:\rname: mypod\rspec:\rcontainers:\r- name: myfrontend\rimage: dockerfile/nginx\rvolumeMounts:\r- mountPath: \"/var/www/html\" name: mypd volumes:\r- name: mypd\rpersistentVolumeClaim:\rclaimName: myclaim Kubernetes Trobleshooting Check the nodes status oc get nodes\rCheck the status of pods oc get pods Check the pods where are they scheduled oc get pod -o wide\rcheck the events for a particular namepsace oc get events\rLook for any error in pod oc logs podname\rDescribe pod if the pod status is pending oc describe pod podname\rHow to login to private image registry docker login\rCreate a secret for imagePull secret oc create secret generic regcred \\\r--from-file=.dockerconfigjson=\u003cpath/to/.docker/config.json\u003e \\\r--type=kubernetes.io/dockerconfigjson\rUse Imagepull secret in pod apiVersion: v1\rkind: Pod\rmetadata:\rname: private-reg\rspec:\rcontainers:\r- name: private-reg-container\rimage: \u003cyour-private-image\u003e\rimagePullSecrets:\r- name: regcred\rCheck the ip address of pod with specifice label oc get pods -l app=hostnames \\\r-o go-template='{{range .items}}{{.status.podIP}}{{\"\\n\"}}{{end}}'\rTry to check the connectivity from a pod for ep in 10.244.0.5:9376 10.244.0.6:9376 10.244.0.7:9376; do\rwget -qO- $ep\rdone",
    "description": "Day02",
    "tags": [],
    "title": "Part 2",
    "uri": "/ocp/part-2/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "description": "How to increase disk size in virtualbox Stop the vm on virtualbox check the disk VBoxManage list hdds\rIncrease the size to 50 GB VBoxManage modifyhd \"E:\\VirtualBox VMs\\crc_default_1725632457848_7611\\Rocky-9-Vagrant-Vbox-9.4-20240509.0.x86_64.vmdk\" --resize 51200",
    "tags": [],
    "title": "Virtual Box",
    "uri": "/ocp/virtualbox/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation¬†\u003e¬†Redhat Openshift",
    "content": "OpenShift Administrators Roles and Responsibilities Updating SSH Keys on OpenShift 4 Nodes via MachineConfig Adding an SSL Certificate to Trusted Store in OpenShift Cluster SSL Related Tasks in OpenShift 4 Administration Updating the Default SSL Certificate Changing Configuration for All OpenShift Nodes Adjusting Kernel Parameters Changing the Default Container Runtime Settings Upgrading OpenShift Cluster Upgrading OpenShift Nodes in Batches How to change the no of node unavailable while upgrading Modifying OpenShift Project Templates Adding Resource Quotas Adding LimitRanges Exposing the Internal OpenShift Registry Overview of the openshift-config Namespace in OpenShift OpenShift Administrators Roles and Responsibilities. Capacity Planning\nProof of concepts\nCluster Installation and Upgrades\nSet up new clusters, apply regular updates, and perform major upgrades. This includes managing Red Hat OpenShift Cluster Manager and leveraging tools like Red Hat Advanced Cluster Management (RHACM) for multi-cluster environments. User Management and Access Control\nImplement role-based access control (RBAC) policies, manage users and permissions, and integrate authentication providers like LDAP or Active Directory. Storage Management\nConfigure persistent storage using OpenShift Container Storage (OCS) or other storage providers. Manage storage classes, Persistent Volumes (PVs), and Persistent Volume Claims (PVCs) to ensure reliable data storage for applications. Networking and Ingress Configuration\nManage networking policies, routes, ingress controllers, and network security configurations. Tasks include setting up cluster networking (SDN or OpenShift SDN), load balancing, and managing multi-cluster networking with RHACM. Monitoring and Logging\nUse Prometheus and Grafana for monitoring, and EFK/ELK stacks (Elasticsearch, Fluentd, Kibana) for centralized logging. Set up alerting for resource utilization, network traffic, and error logs to proactively manage cluster health. Security Compliance and Vulnerability Management\nApply security patches, manage OpenShift‚Äôs integrated security features, and scan images for vulnerabilities. Enable features like the OpenShift Compliance Operator for automated security checks. Resource Optimization and Quota Management\nMonitor and optimize resource usage (CPU, memory), and set quotas and limits for namespaces and projects to prevent resource exhaustion and ensure fair resource allocation. Backup and Disaster Recovery\nImplement backup and restore strategies using tools like Velero for Kubernetes backups, Kasten, or Red Hat-supported backup solutions. DR planning includes setting up automated backups and validating restore procedures regularly. Operator Lifecycle Management (OLM)\nInstall, upgrade, and manage Operators for automating infrastructure components and applications. Ensure Operators are kept up-to-date and compatible with the OpenShift environment. Support and Troubleshooting\nRegularly troubleshoot performance issues, manage logs, resolve configuration errors, and work with Red Hat Support or internal resources for escalations. Common issues include network latency, failed deployments, or resource constraints impacting applications. These tasks keep an OpenShift 4 environment secure, high-performing, and aligned with organizational requirements, especially for production-grade clusters.\nUpdating SSH Keys on OpenShift 4 Nodes via MachineConfig OpenShift 4 uses the Machine Config Operator (MCO) to manage node configurations. To update SSH keys across all master and worker nodes, create a MachineConfig resource to apply the new SSH public key.\nStep 1: Generate a New SSH Key Pair (Optional) If you don‚Äôt already have an SSH key pair, generate a new one on your local machine:\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\rThis command creates two files:\n~/.ssh/id_rsa (private key) ~/.ssh/id_rsa.pub (public key) Step 2: Create a MachineConfig YAML File for SSH Key Update Create a YAML file, e.g., ssh-key-update.yaml, with the following content. Replace \u003cyour-ssh-public-key\u003e with the actual content of your SSH public key.\napiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: ssh-key-update labels: machineconfiguration.openshift.io/role: \u003cnode-role\u003e # Replace \u003cnode-role\u003e with \"worker\" or \"master\" spec: config: ignition: version: 3.2.0 passwd: users: - name: core sshAuthorizedKeys: - \"ssh-rsa \u003cyour-ssh-public-key\u003e\"\rmachineconfiguration.openshift.io/role: Set this to worker to target worker nodes or master to target master nodes. sshAuthorizedKeys: Add your SSH public key here (the content of id_rsa.pub). Step 3: Apply the MachineConfig Apply the MachineConfig to your OCP 4 cluster: oc apply -f ssh-key-update.yaml\rMonitor the rollout status to confirm that the update is applied across nodes: oc get machineconfigpool\rWait until the UPDATED status is True for the respective machineconfigpool (e.g., worker or master).\nStep 4: Verify the SSH Key Update Once the rollout is complete, verify that you can SSH into the nodes as the core user:\nssh core@\u003cnode-ip\u003e\rThis should authenticate using the updated SSH key.\nImportant Notes:\nDo not manually modify SSH keys directly on nodes, as the Machine Config Operator will revert changes outside of its control. Role-based MachineConfig: If you need different keys for master and worker nodes, create separate MachineConfig files, each with the appropriate machineconfiguration.openshift.io/role label. Automation: Changes applied with MachineConfig will persist across reboots and node replacements, aligning with OpenShift‚Äôs immutable infrastructure approach. Adding an SSL Certificate to Trusted Store in OpenShift Cluster To add an SSL certificate to the trusted store in an OpenShift cluster, follow these steps:\nStep 1: Obtain the SSL Certificate Make sure you have the SSL certificate file (e.g., my-certificate.crt) that you want to add to the trusted store.\nStep 2: Create a ConfigMap or Secret You can either use a ConfigMap or a Secret to store the SSL certificate. A Secret is recommended for sensitive data.\nCreate a Secret Create a Secret containing the SSL certificate: oc create secret generic my-ssl-cert --from-file=ca.crt=my-certificate.crt -n \u003cnamespace\u003e\rReplace \u003cnamespace\u003e with the desired namespace where you want to create the secret.\nCreate a ConfigMap (Optional) If you choose to use a ConfigMap, you can create it with:\noc create configmap my-ssl-cert --from-file=my-certificate.crt -n \u003cnamespace\u003e\rStep 3: Configure the Trusted CA To configure the OpenShift cluster to trust the SSL certificate, you need to add it to the appropriate configuration.\nFor Custom Certificates Edit the ClusterIP service to use the custom CA: oc patch configmap/cluster --type merge -p '{\"data\":{\"trust-ca.crt\": \"your-certificate-data\"}}' -n openshift-config\rReplace \"your-certificate-data\" with the actual contents of your SSL certificate.\nStep 4: Update the API Server Edit the API server configuration to include your SSL certificate: oc patch apiserver cluster --type merge --patch '{\"spec\":{\"trustedCA\":{\"name\":\"my-ssl-cert\"}}}'\rStep 5: Restart Affected Pods To apply the changes, you may need to restart the affected pods, such as the API server or any applications that need to trust the new certificate.\nRestart the affected pods: oc rollout restart deployment/\u003cdeployment-name\u003e -n \u003cnamespace\u003e\rStep 6: Verify the Configuration Verify that the certificate has been added to the trusted store: You can check the trusted certificates by accessing a pod and verifying the CA store: oc exec -it \u003cpod-name\u003e -- cat /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\rImportant Notes:\nAlways ensure you have the correct permissions to create secrets and modify configurations in the cluster. Be cautious when handling sensitive information like SSL certificates. Depending on your OpenShift version and setup, some steps may vary slightly. SSL Related Tasks in OpenShift 4 Administration As an OpenShift 4 administrator, managing SSL certificates is crucial for securing communications. This guide outlines common SSL-related tasks, including adding custom certificates, renewing certificates, and managing the default certificate.\nTable of Contents Adding a Custom SSL Certificate Renewing SSL Certificates Updating the Default SSL Certificate Verifying SSL Certificates Adding a Custom SSL Certificate Step 1: Obtain the SSL Certificate Ensure you have the SSL certificate files (e.g., my-certificate.crt and my-key.key).\nStep 2: Create a Secret for the SSL Certificate Create a Secret containing the SSL certificate and key: oc create secret tls my-tls-secret --cert=my-certificate.crt --key=my-key.key -n \u003cnamespace\u003e\rReplace \u003cnamespace\u003e with the desired namespace.\nStep 3: Configure Ingress or Routes to Use the Custom SSL Certificate Edit your Ingress or Route to reference the Secret: apiVersion: route.openshift.io/v1 kind: Route metadata: name: my-route namespace: \u003cnamespace\u003e spec: host: myapp.example.com to: kind: Service name: my-service tls: termination: edge certificate: | -----BEGIN CERTIFICATE----- \u003cyour-certificate-content\u003e -----END CERTIFICATE----- key: | -----BEGIN PRIVATE KEY----- \u003cyour-key-content\u003e -----END PRIVATE KEY----- Renewing SSL Certificates Step 1: Obtain the New SSL Certificate Acquire the new SSL certificate files when it‚Äôs time to renew your certificate.\nStep 2: Update the Existing Secret Update the Secret with the new certificate and key: oc create secret tls my-tls-secret --cert=new-certificate.crt --key=new-key.key -n \u003cnamespace\u003e --dry-run=client -o yaml | oc apply -f -\rStep 3: Verify the Route Ensure that the updated SSL certificate is being used:\noc get route my-route -n \u003cnamespace\u003e -o yaml\rUpdating the Default SSL Certificate Step 1: Create or Update a ConfigMap If you need to update the default certificates for the cluster (e.g., for the API server), create or update a ConfigMap:\noc create configmap custom-ca --from-file=my-certificate.crt -n openshift-config --dry-run=client -o yaml | oc apply -f -\rStep 2: Patch the API Server Patch the API server to use the new custom CA: oc patch apiserver cluster --type merge --patch '{\"spec\":{\"trustedCA\":{\"name\":\"custom-ca\"}}}'\rVerifying SSL Certificates Step 1: Verify SSL Certificate in Use You can verify which SSL certificate is currently in use for a route:\noc get route my-route -n \u003cnamespace\u003e -o jsonpath='{.spec.tls}'\rStep 2: Check the Certificate Chain To check the certificate chain of a service, you can exec into a pod and use tools like openssl:\noc exec -it \u003cpod-name\u003e -- /bin/sh openssl s_client -connect myapp.example.com:443 -showcerts\rImportant Notes:\nEnsure you have the necessary permissions to manage secrets and configurations in OpenShift. Always back up existing certificates and configurations before making changes. Monitor the expiration of certificates and set reminders to renew them in advance. Changing Configuration for All OpenShift Nodes OpenShift 4 uses the Machine Config Operator to manage the configuration of nodes. To change the configuration for all nodes, follow these steps:\nStep 1: Identify the Configuration Change Determine what specific configuration you need to change. Common changes include:\nUpdating SSH keys Modifying systemd services Adjusting kernel parameters Changing the default container runtime settings Step 2: Create a MachineConfig Create a YAML file for the MachineConfig. For example, to update the SSH keys, create a file named update-ssh-keys.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: update-ssh-keys labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 passwd: users: - name: core sshAuthorizedKeys: - \"ssh-rsa \u003cyour-new-ssh-public-key\u003e\"\rReplace \u003cyour-new-ssh-public-key\u003e with the actual content of the new SSH public key.\nApply the MachineConfig: oc apply -f update-ssh-keys.yaml\rStep 3: Monitor the Status of the Machine Config Pool After applying the MachineConfig, you should monitor the status of the affected Machine Config Pool to ensure the changes are applied:\noc get machineconfigpool\rWait until the UPDATED status is True for the respective Machine Config Pool (e.g., worker or master).\nStep 4: Verify the Configuration Change SSH into one of the nodes to verify the configuration change: ssh core@\u003cnode-ip\u003e\rCheck the updated configuration as needed, for example, to verify SSH keys: cat ~/.ssh/authorized_keys\rStep 5: Restart Affected Services or Pods Some changes might require restarting affected services or pods. To restart the affected pods, you can use:\noc rollout restart deployment/\u003cdeployment-name\u003e -n \u003cnamespace\u003e\rImportant Notes Machine Config Operator: Be cautious when using the Machine Config Operator, as incorrect configurations can lead to node failures. Backup Existing Configurations: Always backup your existing configurations before making changes. Node Role: Make sure to specify the correct node role (master or worker) in the labels of your MachineConfig. Multiple Changes: If you have multiple changes to apply, consider creating a single MachineConfig that encapsulates all required changes, rather than creating multiple individual MachineConfigs. This guide provides a structured approach to changing configurations for all OpenShift nodes. If you have specific configurations in mind or need further assistance, feel free to ask!\nChanging Configuration for All OpenShift Nodes OpenShift 4 uses the Machine Config Operator (MCO) to manage the configuration of nodes. This guide provides steps to modify systemd services, adjust kernel parameters, and change default container runtime settings.\nModifying Systemd Services Step 1: Create a MachineConfig for Systemd Modifications Create a YAML file for the MachineConfig. For example, to modify a systemd service, create a file named modify-systemd.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: modify-systemd labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 systemd: units: - name: my-service.service enabled: true dropins: - name: 10-my-custom.conf contents: | [Service] Environment=\"MY_ENV_VAR=my_value\" Replace my-service.service and the Environment variables with your actual service name and configuration.\nStep 2: Apply the MachineConfig oc apply -f modify-systemd.yaml\rStep 3: Verify the Change SSH into the node and check the status of the modified service:\nssh core@\u003cnode-ip\u003e systemctl status my-service.service\rAdjusting Kernel Parameters Step 1: Create a MachineConfig for Kernel Parameters Create a YAML file for the MachineConfig. For example, to adjust kernel parameters, create a file named adjust-kernel-params.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: adjust-kernel-params labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 kernelArguments: - \"parameter1=value1\" - \"parameter2=value2\"\rReplace parameter1=value1 and parameter2=value2 with the actual kernel parameters you need to set.\nStep 2: Apply the MachineConfig oc apply -f adjust-kernel-params.yaml\rStep 3: Verify the Change SSH into the node and check the current kernel parameters:\nssh core@\u003cnode-ip\u003e sysctl -a | grep parameter1\rChanging the Default Container Runtime Settings Step 1: Create a MachineConfig for Container Runtime Settings Create a YAML file for the MachineConfig. For example, to change container runtime settings, create a file named change-runtime-settings.yaml with the following content: apiVersion: machineconfiguration.openshift.io/v1 kind: MachineConfig metadata: name: change-runtime-settings labels: machineconfiguration.openshift.io/role: worker # or master spec: config: ignition: version: 3.2.0 storage: files: - path: /etc/crio/crio.conf mode: 0644 owner: root:root contents: inline: | [crio] default_runtime = \"my-runtime\" [runtime] runtime_type = \"my-runtime\" Replace my-runtime with your actual runtime settings.\nStep 2: Apply the MachineConfig oc apply -f change-runtime-settings.yaml\rStep 3: Verify the Change SSH into the node and check the runtime configuration:\nssh core@\u003cnode-ip\u003e cat /etc/crio/crio.conf\rStep 4: Monitor the Machine Config Pool After applying any of the MachineConfigs, monitor the Machine Config Pool:\noc get machineconfigpool\rWait until the UPDATED status is True for the respective Machine Config Pool (e.g., worker or master).\nImportant Notes:\nMachine Config Operator: Be cautious when using the MCO, as incorrect configurations can lead to node failures. Backup Existing Configurations: Always backup your existing configurations before making changes. Node Role: Ensure to specify the correct node role (master or worker) in the labels of your MachineConfig. Service Impact: Some changes may require restarting the affected services or pods for the changes to take effect. This guide provides clear instructions on modifying systemd services, adjusting kernel parameters, and changing container runtime settings in OpenShift nodes. If you have specific requirements or additional questions, feel free to ask!\nUpgrading OpenShift Cluster Upgrading an OpenShift cluster requires careful planning and execution. This guide outlines the steps for upgrading an OpenShift cluster.\nPrerequisites Backup Important Data:\nBackup your current cluster state, including etcd data and any custom resources. Review Release Notes:\nCheck the OpenShift Release Notes for any specific upgrade instructions and deprecated features. Check Compatibility:\nVerify that your current version is compatible with the target version. Update CLI Tools:\nEnsure you have the latest version of oc and openshift-install CLI tools that match the version you‚Äôre upgrading to. Step 1: Prepare for the Upgrade Check Cluster Health:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rDrain Nodes:\nStart with the master nodes, draining them to prepare for the upgrade. oc adm drain \u003cmaster-node-name\u003e --ignore-daemonsets --force --delete-local-data\rUpdate Machine Configurations (if needed):\nIf there are any machine configurations that need updates before the upgrade, apply those changes now. Step 2: Upgrade the Cluster Start the Upgrade Process:\nTrigger the upgrade process using the OpenShift CLI. oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e\rMonitor the Upgrade:\nCheck the status of the upgrade process. oc get clusterversion\rAllow Upgrade to Complete:\nWait for the upgrade process to complete. Ensure that all cluster operators are reporting as Available. Step 3: Post-Upgrade Tasks Uncordon Nodes:\nOnce the upgrade is complete, uncordon the master nodes and worker nodes. oc adm uncordon \u003cmaster-node-name\u003e oc adm uncordon \u003cworker-node-name\u003e\rCheck Cluster Health Again:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rUpdate Machine Configurations (if necessary):\nIf there are additional machine configurations or settings required for the new version, apply those now. Validate Application Functionality:\nVerify that all applications are functioning as expected after the upgrade. Step 4: Clean Up Remove Old Images:\nClean up unused images from the nodes to free up space. oc adm prune images\rMonitor Cluster Performance:\nKeep an eye on the cluster performance after the upgrade to ensure everything is running smoothly. Important Notes Upgrade Path: Always follow the recommended upgrade path (e.g., from 4.x to 4.y) as specified in the OpenShift documentation. Test Upgrades: If possible, test the upgrade process in a staging environment before performing it in production. Documentation: Refer to the official OpenShift Upgrade Documentation for detailed instructions specific to your version. This guide provides a structured approach to upgrading your OpenShift cluster. If you have specific requirements or encounter issues during the upgrade process, feel free to ask for assistance!\nUpgrading OpenShift Nodes in Batches When upgrading an OpenShift cluster with multiple nodes, you can upgrade nodes in batches (e.g., three at a time) to minimize downtime and ensure a smooth transition.\nPrerequisites Backup Your Cluster: Before proceeding, back up your etcd and any critical application data. Review Release Notes: Check the OpenShift release notes for any breaking changes or important notes about the upgrade. Upgrade Plan: Decide on the upgrade order for your nodes (typically, master nodes are upgraded first, followed by worker nodes). Step 1: Prepare for the Upgrade Check Cluster Health:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rDrain the Nodes:\nStart with the nodes you plan to upgrade. Drain three nodes at a time. for node in node1 node2 node3; do oc adm drain $node --ignore-daemonsets --force --delete-local-data done\rReplace node1, node2, and node3 with the actual node names.\nStep 2: Upgrade the Nodes Start the Upgrade Process:\nBegin the upgrade for the selected nodes. for node in node1 node2 node3; do oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e --node=$node done\rReplace \u003cnew-openshift-image\u003e with the image for the target OpenShift version.\nMonitor the Upgrade:\nCheck the status of the upgrade process for each node. oc get clusterversion\rWait until the upgrade process completes for all three nodes.\nStep 3: Uncordon the Nodes Uncordon the Upgraded Nodes: Once the upgrade is complete, uncordon the nodes to make them schedulable again. for node in node1 node2 node3; do oc adm uncordon $node done\rStep 4: Repeat for Remaining Nodes Continue Upgrading: Repeat the above steps for the next batch of three nodes until all nodes are upgraded. # Upgrade next batch for node in node4 node5 node6; do oc adm drain $node --ignore-daemonsets --force --delete-local-data oc adm upgrade begin --to-image=\u003cnew-openshift-image\u003e --node=$node oc adm uncordon $node done\rStep 5: Post-Upgrade Verification Check Cluster Health Again:\noc get nodes oc get pods --all-namespaces oc get clusteroperators\rValidate Application Functionality:\nEnsure that all applications are functioning as expected after the upgrade. Important Notes Plan for Downtime: Upgrading nodes in batches may cause temporary unavailability for applications running on those nodes. Ensure your applications are resilient and can handle node failures. Monitor Performance: After upgrading, monitor cluster performance and health metrics to ensure everything is running smoothly. Follow Upgrade Path: Always adhere to the recommended upgrade paths specified in the OpenShift documentation. By following this guide, you can effectively upgrade three nodes at a time in your OpenShift cluster. If you have any specific requirements or encounter issues during the upgrade process, feel free to ask for assistance!\nHow to change the no of node unavailable while upgrading oc patch mcp worker --type merge --patch '{\"spec\": {\"maxUnavailable\": 2}}'\rVerify the maxUnavailable value: oc get mcp worker -o yaml | grep maxUnavailable\rModifying OpenShift Project Templates OpenShift provides the ability to create and modify project templates, which define a standard set of resources and configurations that can be instantiated to create new projects or applications quickly. Modifying a project template allows you to customize default behaviors and settings for your applications.\nKey Components of a Project Template Parameters: Variables that can be customized when instantiating the template. Objects: The resources that will be created when the template is applied (e.g., DeploymentConfigs, Services, Routes). Labels and Annotations: Metadata for categorizing and describing resources. Viewing Existing Templates To view the templates available in your OpenShift environment, use:\noc get templates\rModifying a Template You can modify an existing template by following these steps:\n1. Export the Template To export a template to a file for editing:\noc get template \u003ctemplate-name\u003e -o yaml \u003e template.yaml\rReplace \u003ctemplate-name\u003e with the name of the template you want to modify.\n2. Edit the Template Open the template.yaml file in your preferred text editor and make the necessary changes. Key areas to consider modifying:\nParameters: Add, remove, or change parameter definitions. Objects: Modify or add resource specifications (e.g., change replicas in a DeploymentConfig). Labels/Annotations: Update or add labels and annotations for better resource management. 3. Apply the Modified Template Once you‚Äôve made your modifications, you can apply the updated template back to OpenShift:\noc apply -f template.yaml\rAdding Resource Quotas Resource quotas are used to limit the amount of resources that can be consumed in a project. To add a resource quota to your template, include a ResourceQuota object in the objects section of the template:\napiVersion: v1 kind: ResourceQuota metadata: name: example-quota spec: hard: requests.cpu: \"4\" requests.memory: \"8Gi\" limits.cpu: \"4\" limits.memory: \"8Gi\"\rAdding LimitRanges Limit ranges set default request and limit values for containers in a project. You can add a LimitRange object in the objects section of the template:\napiVersion: v1 kind: LimitRange metadata: name: example-limits spec: limits: - default: cpu: \"500m\" memory: \"1Gi\" defaultRequest: cpu: \"250m\" memory: \"512Mi\" type: Container\rCreating a New Template If you want to create a new project template from scratch, you can use:\noc create -f \u003ctemplate-file\u003e.yaml\rMake sure to define parameters, objects, and metadata in the YAML file.\nExample Template Structure Here‚Äôs a simple example structure of a project template in YAML, including resource quotas and limit ranges:\napiVersion: template.openshift.io/v1 kind: Template metadata: name: example-template labels: app: example parameters: - name: APP_NAME description: The name of the application required: true objects: - apiVersion: v1 kind: ResourceQuota metadata: name: example-quota spec: hard: requests.cpu: \"4\" requests.memory: \"8Gi\" limits.cpu: \"4\" limits.memory: \"8Gi\" - apiVersion: v1 kind: LimitRange metadata: name: example-limits spec: limits: - default: cpu: \"500m\" memory: \"1Gi\" defaultRequest: cpu: \"250m\" memory: \"512Mi\" type: Container - apiVersion: apps/v1 kind: Deployment metadata: name: ${APP_NAME} spec: replicas: 1 selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} spec: containers: - name: ${APP_NAME} image: nginx ports: - containerPort: 80\rConclusion Modifying OpenShift project templates is a powerful way to standardize application deployments and ensure consistency across projects. By using parameters, object definitions, resource quotas, and limit ranges effectively, you can create flexible and reusable templates tailored to your organization‚Äôs needs.\nConfiguring a Node Selector for a Project oc adm new-project demo --node-selector \"tier=1\" oc annotate namespace demo openshift.io/node-selector=\"tier=2\" --overwrite\rApplying Quotas to Multiple Projects The following is an example of creating a cluster resource quota for all projects owned by the qa user: oc create clusterquota user-qa \\ \u003e --project-annotation-selector openshift.io/requester=qa \\ \u003e --hard pods=12,secrets=20\rThe following is an example of creating a cluster resource quota for all projects that have been assigned the environment=qa label: oc create clusterquota env-qa \\ \u003e --project-label-selector environment=qa \\ \u003e --hard pods=10,services=5\rExposing the Internal OpenShift Registry To expose the internal OpenShift registry externally:\nVerify Registry Status oc get pods -n openshift-image-registry\rExpose the Registry Service oc expose service image-registry -n openshift-image-registry --name=external-registry-route\rRetrieve the Route URL oc get route external-registry-route -n openshift-image-registry # Output should be a URL like: https://external-registry-route.\u003ccluster-domain\u003e\rLog in to the Registry using oc CLI oc login -u \u003cusername\u003e -p \u003cpassword\u003e oc registry login\rAlternatively, log in using Docker CLI oc whoami -t # Retrieve token docker login -u \u003cusername\u003e -p \u003ctoken\u003e https://external-registry-route.\u003ccluster-domain\u003e\rPush and Pull Images Push docker tag \u003cimage\u003e external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e docker push external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e\rPull docker pull external-registry-route.\u003ccluster-domain\u003e/\u003cproject\u003e/\u003cimage\u003e\rOverview of the openshift-config Namespace in OpenShift The openshift-config namespace in OpenShift is a critical system namespace that stores configuration resources essential for the cluster‚Äôs operation, including settings for authentication, networking, and security.\nKey Resources in openshift-config 1. Authentication and OAuth Configuration OAuth (OAuth CR): Manages user authentication for the cluster. Identity Providers: Configuration for providers like LDAP, GitHub, Google, OpenID Connect, etc. OAuth Tokens: Settings for tokens used by applications or users to authenticate with the API. 2. Ingress and Proxy Configuration Cluster Proxy (Proxy CR): Cluster-wide HTTP/HTTPS proxy settings for clusters behind firewalls or with limited internet access. Cluster Ingress (Ingress CR): Default settings for external application exposure and the default domain for routes. 3. Network Configuration DNS ConfigMap: Custom DNS configuration required by cluster applications. Cluster-wide Network Settings: Configures network policies, network isolation, and other networking settings. 4. Certificate and Secret Management Certificates for Internal Services: TLS certificates for internal cluster services like the API server and internal registry. Trusted CA Bundle: CA bundle configuration for trusting external/internal certificates. 5. Image Registry Configuration Registry ConfigMap: Settings for the internal OpenShift image registry (e.g., storage backend and access control). Image Content Source Policies: Policies controlling allowed registries for image pulling and storage. 6. Etcd Configuration Etcd Settings: Custom settings for the etcd data store, critical for OpenShift‚Äôs cluster state and configuration. 7. Cluster Operators Configuration Configuration for various cluster operators affecting core components like networking, ingress, and monitoring. 8. Custom Configurations for OpenShift Components Holds configurations for specific deployment needs, security hardening, or custom operational policies. Summary of Key Resource Types in openshift-config ConfigMaps Secrets Custom Resources: Like OAuth, Ingress, and Proxy These configurations in openshift-config are essential for managing the functionality, security, and accessibility of the OpenShift cluster.",
    "description": "Openshift",
    "tags": [],
    "title": "Openshift 4 Tasks",
    "uri": "/ocp/ocp-tasks/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "breadcrumb": "Openshift Documentation",
    "content": "",
    "description": "",
    "tags": [],
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
